from fastapi import FastAPI, HTTPException, Depends, status, Body, UploadFile, File
from fastapi.security import OAuth2PasswordRequestForm
from fastapi.middleware.cors import CORSMiddleware
from fastapi.staticfiles import StaticFiles
from fastapi.responses import RedirectResponse
from pydantic import BaseModel, Field
from typing import List, Optional, Dict, Any
import uvicorn
from datetime import datetime, timedelta
from pathlib import Path
import fitz  # PyMuPDF

# Import services
from services.chat import ChatService
from services.summarize import SummarizeService
from services.email_drafter import EmailDraftService
from services.todo_manager import TodoManager
from services.translator import TranslatorService
from services.code_helper import CodeHelperService

# Import utilities
from utils.llm_client import LLMClient
from utils.file_handler import FileHandler
from utils.logger import logger, log_request, log_response, log_error
from utils.auth import (
    Token, User, authenticate_user, create_access_token,
    get_current_active_user, ACCESS_TOKEN_EXPIRE_MINUTES,
    rate_limiter, UserInDB
)

# --- API Documentation Details ---
API_DESCRIPTION = """
Personal AI Assistant API helps you leverage Large Language Models for various tasks. 

Capabilities include:
*   **Chat:** Engage in conversations.
*   **Summarize:** Summarize text or documents (PDF/TXT).
*   **Email:** Draft emails based on prompts.
*   **Todo:** Manage your tasks.
*   **Translate:** Translate text between languages.
*   **Code:** Get help with programming code.

Requires authentication via JWT tokens obtained from the /token endpoint.
"""

API_TAGS_METADATA = [
    {
        "name": "Authentication",
        "description": "Operations for user authentication and token management.",
    },
    {
        "name": "Assistant",
        "description": "Core AI assistant operations for different task types.",
    },
    {
        "name": "Utilities",
        "description": "General utility endpoints.",
    },
]

app = FastAPI(
    title="LLM Personal Assistant",
    description=API_DESCRIPTION,
    version="1.0.0",
    openapi_tags=API_TAGS_METADATA,
    contact={
        "name": "AI Assistant Team",
        "url": "http://example.com/contact", # Replace with actual contact
        "email": "support@example.com",      # Replace with actual email
    },
    license_info={
        "name": "Apache 2.0",
        "url": "https://www.apache.org/licenses/LICENSE-2.0.html",
    },
)

# Configure CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*", "http://localhost:3002"],  # Adjust for production (e.g., ["https://your-app-name.fly.dev"])
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# --- Mount Static Files --- 
static_dir = Path(__file__).parent / "static"
# Check if static directory exists, create if not (useful for local dev)
if not static_dir.exists():
    static_dir.mkdir()
    (static_dir / "js").mkdir()
    print("Created static/ and static/js/ directories.")

app.mount("/static", StaticFiles(directory=static_dir), name="static")
# Serve index.html at the root (or another path)
app.mount("/ui", StaticFiles(directory=static_dir, html=True), name="ui_root")

# Initialize services
llm_client = LLMClient()
file_handler = FileHandler()

chat_service = ChatService(llm_client)
summarize_service = SummarizeService(llm_client, file_handler)
email_service = EmailDraftService(llm_client)
todo_service = TodoManager()
translator_service = TranslatorService(llm_client)
code_service = CodeHelperService(llm_client)

# Add shutdown event to close the LLM client
@app.on_event("shutdown")
async def shutdown_event():
    logger.info("Closing LLM client...")
    await llm_client.close()
    logger.info("LLM client closed.")

# --- Pydantic Models with Examples ---
class Message(BaseModel):
    content: Optional[str] = Field(None, description="Text content for the request (required unless filename is provided for summarize)", example="Explain the theory of relativity.")
    type: str = Field(..., description="Type of assistant request", examples=["chat", "summarize", "email", "todo", "translate", "code"])
    parameters: Optional[Dict[str, Any]] = Field(None, description="Optional parameters specific to the request type", example={"filename": "mydoc.pdf", "format": "bullets"})

class Response(BaseModel):
    content: str = Field(..., description="The primary content generated by the assistant", example="E=mc^2, which means energy equals mass times the speed of light squared...")
    metadata: Optional[Dict[str, Any]] = Field(None, description="Additional metadata about the response (e.g., model used, usage tokens, detected language)", example={"model": "mistralai/mistral-7b-instruct:free", "usage": {"total_tokens": 50}})

# --- API Endpoints with Enhanced Docs ---

@app.post("/token", response_model=Token, tags=["Authentication"])
async def login_for_access_token(form_data: OAuth2PasswordRequestForm = Depends()):
    """
    Authenticates a user and returns an access token.
    
    Use standard OAuth2 form data (username, password).
    """
    user = authenticate_user(form_data.username, form_data.password)
    if not user:
        raise HTTPException(
            status_code=status.HTTP_401_UNAUTHORIZED,
            detail="Incorrect username or password",
            headers={"WWW-Authenticate": "Bearer"},
        )
    access_token_expires = timedelta(minutes=ACCESS_TOKEN_EXPIRE_MINUTES)
    access_token = create_access_token(
        data={"sub": user.username}, expires_delta=access_token_expires
    )
    return {"access_token": access_token, "token_type": "bearer"}

@app.get("/", include_in_schema=False) # Keep original API root at /api or similar if needed
async def read_root():
    return RedirectResponse(url="/ui")

@app.get("/users/me", response_model=User, tags=["Authentication"])
async def read_users_me(current_user: User = Depends(get_current_active_user)):
    """Returns the details of the currently authenticated user."""
    return current_user

@app.post("/assist", 
            response_model=Response, 
            tags=["Assistant"], 
            summary="Process Assistant Request",
            description="Send a request to the appropriate AI assistant based on the 'type' field.")
async def process_request(
    message: Message,
    current_user: User = Depends(get_current_active_user)
):
    """
    Main endpoint to interact with the AI assistant.
    
    - **Requires authentication.**
    - Specify the `type` of task (chat, summarize, etc.).
    - Provide `content` or specify parameters like `filename` for relevant types.
    - Check the specific service implementation for supported `parameters`.
    """
    print("ðŸ“¥ Received request to /assist:", message.dict())
    # Rate limiting check
    if not rate_limiter.is_allowed(current_user.username):
        raise HTTPException(
            status_code=status.HTTP_429_TOO_MANY_REQUESTS,
            detail="Too many requests"
        )

    try:
        # Log request
        log_request(
            service=message.type,
            action="process",
            content=message.content if message.content else "(file input)", # Adjust logging
            parameters=message.parameters
        )

        # Process request based on type
        response = None
        if message.type == "summarize":
            response = await summarize_service.process(message.content, message.parameters)
        elif message.type == "email":
            response = await email_service.process(message.content, message.parameters)
        elif message.type == "todo":
            response = await todo_service.process(message.content, message.parameters)
        elif message.type == "code":
            response = await code_service.process(message.content, message.parameters)
        elif message.type == "translate":
            response = await translator_service.process(message.content, message.parameters)
        elif message.type == "chat":
            response = await chat_service.process(message.content, message.parameters)
        else:
            # Log invalid type attempt
            logger.warning(f"Invalid request type received: {message.type}")
            raise HTTPException(status_code=400, detail=f"Invalid request type: {message.type}")

        # Log response
        log_response(
            service=message.type,
            action="process",
            status="success",
            metadata=response.get("metadata")
        )

        return response

    except Exception as e:
        # Log error
        log_error(
            service=message.type,
            action="process",
            error=e,
            context={"content": message.content, "parameters": message.parameters}
        )
        # Consider returning a more structured error response
        raise HTTPException(status_code=500, detail=f"An internal error occurred: {str(e)}")


@app.get("/todos", tags=["Assistant"])
async def list_todos(current_user: User = Depends(get_current_active_user)):
    return todo_service.list_tasks(current_user.username)

@app.post("/todos", tags=["Assistant"])
async def add_todo(task: str = Body(..., embed=True), current_user: User = Depends(get_current_active_user)):
    return todo_service.add_task(current_user.username, task)

@app.put("/todos/{task_id}", tags=["Assistant"])
async def update_todo(task_id: int, task: str = Body(..., embed=True), current_user: User = Depends(get_current_active_user)):
    return todo_service.update_task(current_user.username, task_id, task)

@app.delete("/todos/{task_id}", tags=["Assistant"])
async def delete_todo(task_id: int, current_user: User = Depends(get_current_active_user)):
    return todo_service.delete_task(current_user.username, task_id)

if __name__ == "__main__":
    logger.info("Starting LLM Assistant API")
    uvicorn.run("main:app", host="0.0.0.0", port=8000, reload=True)
