"""
LLM Personal Assistant API - FastAPI Backend
------------------------------------------
Main application entry point with API route definitions and middleware configuration.
Follows a clean, modular architecture with separate route, controller, and service layers.

Author: AI Assistant Team
Version: 1.0.0
"""

from fastapi import FastAPI, HTTPException, Depends, status, Body, File, UploadFile, Request, Response
from fastapi.security import OAuth2PasswordRequestForm, OAuth2PasswordBearer
from fastapi.middleware.cors import CORSMiddleware
from fastapi.staticfiles import StaticFiles
from fastapi.responses import RedirectResponse, JSONResponse
from fastapi.middleware.gzip import GZipMiddleware
from pydantic import BaseModel, Field, validator
from typing import List, Optional, Dict, Any, Union
import uvicorn
from datetime import datetime, timedelta
from pathlib import Path
import time
import os
import json
import fitz  # PyMuPDF

# Import services
from services.chat import ChatService, ChatRequest
from services.summarize import SummarizeService
from services.email_drafter import EmailDraftService
from services.todo_manager import TodoManager
from services.translator import TranslatorService
from services.code_helper import CodeHelperService

# Import utilities
from utils.gemini_client import GeminiClient
from utils.file_handler import FileHandler
from utils.logger import logger, log_request, log_response, log_error
from utils.auth import (
    Token, User, authenticate_user, create_access_token,
    get_current_active_user, ACCESS_TOKEN_EXPIRE_MINUTES,
    rate_limiter, UserInDB
)

# --- API Documentation Details ---
API_DESCRIPTION = """
# Personal AI Assistant API

This API helps you leverage Large Language Models for various productivity tasks.

## Core Capabilities:
* üí¨ **Chat:** Engage in natural language conversations
* üìù **Summarize:** Condense text or documents (PDF/TXT)
* ‚úâÔ∏è **Email:** Generate professional email drafts
* ‚úì **Todo:** Manage your tasks and to-do items
* üåê **Translate:** Convert text between languages
* üíª **Code:** Get programming assistance

## Authentication
All endpoints require authentication via JWT tokens obtained from the `/token` endpoint.
"""

API_TAGS_METADATA = [
    {
        "name": "Authentication",
        "description": "Operations for user authentication and token management.",
    },
    {
        "name": "Assistant",
        "description": "Core AI assistant operations for different task types.",
    },
    {
        "name": "Todo Management",
        "description": "Todo list operations (create, read, update, delete).",
    },
    {
        "name": "Utilities",
        "description": "General utility endpoints.",
    },
]

# --- API Models with Validation ---
class Message(BaseModel):
    """
    Input message model for assistant requests with validation.
    """
    content: Optional[str] = Field(None, 
        description="Text content for the request", 
        example="Explain the theory of relativity."
    )
    type: str = Field(..., 
        description="Type of assistant request", 
        examples=["chat", "summarize", "email", "todo", "translate", "code"]
    )
    parameters: Optional[Dict[str, Any]] = Field(
        default={}, 
        description="Optional parameters specific to the request type", 
        example={"filename": "mydoc.pdf", "format": "bullets"}
    )
    
    @validator('type')
    def validate_type(cls, v):
        valid_types = ["chat", "summarize", "email", "todo", "translate", "code"]
        if v not in valid_types:
            raise ValueError(f"Type must be one of: {', '.join(valid_types)}")
        return v
    
    @validator('content')
    def content_or_parameters_required(cls, v, values):
        # Either content or non-empty parameters must be provided
        if v is None and (not values.get('parameters') or len(values.get('parameters', {})) == 0):
            raise ValueError("Either content or parameters must be provided")
        return v

class Response(BaseModel):
    """
    Standard response model with consistent structure.
    """
    content: str = Field(..., 
        description="The primary content generated by the assistant", 
        example="E=mc¬≤ means energy equals mass times the speed of light squared..."
    )
    metadata: Optional[Dict[str, Any]] = Field(
        default={}, 
        description="Additional metadata about the response", 
        example={"model": "gpt-3.5-turbo", "usage": {"total_tokens": 150}}
    )

class ErrorResponse(BaseModel):
    """
    Standardized error response model.
    """
    status: str = "error"
    message: str
    code: int
    details: Optional[Dict[str, Any]] = None

# --- Performance Middleware ---
class PerformanceMiddleware:
    """
    Middleware to track and log API response times.
    """
    async def __call__(self, request: Request, call_next):
        start_time = time.time()
        response = await call_next(request)
        process_time = time.time() - start_time
        
        # Add performance metrics to header
        response.headers["X-Process-Time"] = str(process_time)
        
        # Log performance for non-static requests
        if not request.url.path.startswith("/static") and not request.url.path.startswith("/ui"):
            logger.info(f"Request: {request.method} {request.url.path} - Time: {process_time:.4f}s")
        
        return response

# --- Initialize App with Config ---
app = FastAPI(
    title="LLM Personal Assistant",
    description=API_DESCRIPTION,
    version="1.0.0",
    openapi_tags=API_TAGS_METADATA,
    docs_url="/api/docs",  # Place docs at /api/docs for clarity
    redoc_url="/api/redoc",
    contact={
        "name": "AI Assistant Team",
        "url": "https://github.com/yourusername/personal-ai-assistant",
        "email": "support@example.com",
    },
    license_info={
        "name": "Apache 2.0",
        "url": "https://www.apache.org/licenses/LICENSE-2.0.html",
    },
)

# --- Middleware Registration ---

# Add performance middleware
app.middleware("http")(PerformanceMiddleware())

# Add compression middleware
app.add_middleware(GZipMiddleware, minimum_size=1000)

# Configure CORS for development
app.add_middleware(
    CORSMiddleware,
    allow_origins=[
        "http://localhost:3000",
        "https://personal-ai-agent-beta.vercel.app",
        "https://aiagentbysai.vercel.app"
    ],  # Allow local dev and deployed frontend
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
    expose_headers=["*"],
)

# --- Static Files ---
static_dir = Path(__file__).parent / "static"
# Ensure static directories exist
static_dir.mkdir(exist_ok=True)
(static_dir / "js").mkdir(exist_ok=True)

app.mount("/static", StaticFiles(directory=static_dir), name="static")
app.mount("/ui", StaticFiles(directory=static_dir, html=True), name="ui_root")

# --- Service Initialization ---
gemini_client = GeminiClient()
file_handler = FileHandler()

chat_service = ChatService(gemini_client)
summarize_service = SummarizeService(gemini_client, file_handler)
email_service = EmailDraftService(gemini_client)
todo_service = TodoManager()
translator_service = TranslatorService(gemini_client)
code_service = CodeHelperService(gemini_client)

# --- Lifecycle Events ---
@app.on_event("startup")
async def startup_event():
    """Initialize resources on application startup."""
    logger.info("üöÄ Starting LLM Assistant API")
    # Could initialize database connections, caches, etc.

@app.on_event("shutdown")
async def shutdown_event():
    """Clean up resources on application shutdown."""
    logger.info("Closing LLM client...")
    await gemini_client.close()
    logger.info("LLM client closed. Application shutting down.")

# --- Error Handlers ---
@app.exception_handler(404)
async def not_found_handler(request: Request, exc: HTTPException):
    """Custom 404 handler with JSON response."""
    return JSONResponse(
        status_code=404,
        content=ErrorResponse(
            message="Resource not found",
            code=404,
            details={"path": request.url.path}
        ).dict()
    )

@app.exception_handler(500)
async def server_error_handler(request: Request, exc: Exception):
    """Custom 500 handler with JSON response and error logging."""
    error_id = log_error(
        service="api",
        action="process_request",
        error=exc,
        context={"path": request.url.path, "method": request.method}
    )
    
    return JSONResponse(
        status_code=500,
        content=ErrorResponse(
            message="Internal server error",
            code=500,
            details={"error_id": error_id}
        ).dict()
    )

# --- Auth Routes ---
@app.post(
    "/token", 
    response_model=Token, 
    tags=["Authentication"],
    summary="Get access token",
    response_description="Access token for API operations"
)
async def login_for_access_token(form_data: OAuth2PasswordRequestForm = Depends()):
    """
    Authenticate user and issue a JWT access token.
    
    - Uses standard OAuth2 password flow
    - Returns a bearer token for authenticating subsequent requests
    """
    user = authenticate_user(form_data.username, form_data.password)
    if not user:
        logger.warning(f"Failed login attempt for user: {form_data.username}")
        raise HTTPException(
            status_code=status.HTTP_401_UNAUTHORIZED,
            detail="Incorrect username or password",
            headers={"WWW-Authenticate": "Bearer"},
        )
    
    logger.info(f"Successful login for user: {form_data.username}")
    access_token_expires = timedelta(minutes=ACCESS_TOKEN_EXPIRE_MINUTES)
    access_token = create_access_token(
        data={"sub": user.username}, expires_delta=access_token_expires
    )
    return {"access_token": access_token, "token_type": "bearer"}

# Add a second route for /api/token for compatibility
@app.post(
    "/api/token", 
    response_model=Token, 
    tags=["Authentication"],
    summary="Get access token (alt route)",
    response_description="Access token for API operations"
)
async def login_for_access_token_alt(form_data: OAuth2PasswordRequestForm = Depends()):
    return await login_for_access_token(form_data)

@app.get(
    "/users/me", 
    response_model=User, 
    tags=["Authentication"],
    summary="Get current user details",
)
async def read_users_me(current_user: User = Depends(get_current_active_user)):
    """Returns the profile details of the currently authenticated user."""
    return current_user

# --- Redirection Route ---
@app.get("/", include_in_schema=False)
async def read_root():
    """Redirect root path to UI."""
    return RedirectResponse(url="/ui")

# --- Assistant Routes ---
@app.post(
    "/assist", 
    response_model=Response, 
    tags=["Assistant"], 
    summary="Process any assistant request",
    description="Unified endpoint for all AI assistant capabilities",
    response_description="Assistant-generated content with metadata"
)
async def process_request(
    message: Message,
    current_user: User = Depends(get_current_active_user)
):
    """
    Primary endpoint to interact with the AI assistant.
    
    ## Authentication
    - Requires a valid JWT token
    
    ## Request format
    - Specify the `type` of task (chat, summarize, etc.)
    - Provide `content` text or relevant `parameters`
    
    ## Supported types
    - **chat**: General conversation
    - **summarize**: Text summarization
    - **email**: Email drafting
    - **todo**: To-do management
    - **translate**: Language translation
    - **code**: Programming assistance
    """
    try:
        if message.type == "chat":
            # Build ChatRequest object from message
            chat_request = ChatRequest(
                message=message.content,
                conversation_history=message.parameters.get("conversation_history", []) if message.parameters else [],
                temperature=message.parameters.get("temperature", 0.7) if message.parameters else 0.7
            )
            response = await chat_service.process(chat_request)
        elif message.type == "summarize":
            response = await summarize_service.process(message.content, message.parameters)
        elif message.type == "email":
            response = await email_service.process(message.content, message.parameters)
        elif message.type == "todo":
            response = await todo_service.process(current_user.username, message.content, message.parameters)
        elif message.type == "translate":
            response = await translator_service.process(message.content, message.parameters)
        elif message.type == "code":
            response = await code_service.process(message.content, message.parameters)
        else:
            logger.warning(f"Invalid request type received: {message.type}")
            raise HTTPException(status_code=400, detail=f"Invalid request type: {message.type}")
        
        # Post-process and return
        if isinstance(response, dict):
            if "metadata" not in response:
                response["metadata"] = {}
            response["metadata"]["request_type"] = message.type
            response["metadata"]["timestamp"] = datetime.now().isoformat()
            content = response.get("content", "")
            metadata = response.get("metadata", {})
        else:
            if not getattr(response, "metadata", None):
                response.metadata = {}
            response.metadata["request_type"] = message.type
            response.metadata["timestamp"] = datetime.now().isoformat()
            content = getattr(response, "content", "")
            metadata = getattr(response, "metadata", {})

        log_response(
            service=message.type,
            action="process",
            status="success",
            metadata=metadata
        )
        return Response(content=content, metadata=metadata)

    except Exception as e:
        error_id = log_error(
            service=message.type,
            action="process",
            error=e,
            context={"content": message.content, "parameters": message.parameters}
        )
        raise HTTPException(
            status_code=500, 
            detail=f"An error occurred processing your {message.type} request (Error ID: {error_id})"
        )

# Add a second route for /api/assist for compatibility
@app.post(
    "/api/assist", 
    response_model=Response, 
    tags=["Assistant"], 
    summary="Process any assistant request (alt route)",
    description="Unified endpoint for all AI assistant capabilities (alt)",
    response_description="Assistant-generated content with metadata"
)
async def process_request_alt(
    message: Message,
    current_user: User = Depends(get_current_active_user)
):
    return await process_request(message, current_user)

# --- Todo Management Routes ---
@app.get(
    "/todos", 
    tags=["Todo Management"],
    summary="List all todos",
    response_description="List of user's todo items"
)
async def list_todos(current_user: User = Depends(get_current_active_user)):
    """
    Retrieve all todo items for the authenticated user.
    
    Returns a list of todo items with their IDs, text content, and completion status.
    """
    return await todo_service.list_tasks(current_user.username)

@app.post(
    "/todos", 
    tags=["Todo Management"],
    summary="Create new todo",
    status_code=201,
    response_description="The created todo item"
)
async def add_todo(
    task: str = Body(..., embed=True, description="The todo item text content"),
    current_user: User = Depends(get_current_active_user)
):
    """
    Create a new todo item.
    
    - Requires authentication
    - Accepts a text string as the task description
    - Returns the created task with an assigned ID
    """
    return await todo_service.add_task(current_user.username, task)

@app.put(
    "/todos/{task_id}", 
    tags=["Todo Management"],
    summary="Update a todo",
    response_description="The updated todo item"
)
async def update_todo(
    task_id: int,
    task: str = Body(..., embed=True, description="The updated todo text content"),
    current_user: User = Depends(get_current_active_user)
):
    """
    Update an existing todo item.
    
    - Specify the task_id in the URL path
    - Provide the new task text in the request body
    - Returns the updated task
    """
    # Assume update_task is updated to per-user as well
    return await todo_service.update_task(current_user.username, task_id, task)

@app.delete(
    "/todos/{task_id}", 
    tags=["Todo Management"],
    summary="Delete a todo",
    response_description="Success message"
)
async def delete_todo(
    task_id: int,
    current_user: User = Depends(get_current_active_user)
):
    """
    Delete a todo item.
    
    - Specify the task_id to delete in the URL path
    - Returns a success confirmation
    """
    return await todo_service.delete_task(current_user.username, task_id)

# --- LLM Connectivity Test Route ---
from fastapi import APIRouter
from utils.gemini_client import GeminiClient

@app.get("/llm-test")
async def llm_test():
    # Dummy response to restore backend responsiveness
    return {"status": "success", "response": {"content": "[DUMMY] Hola!", "model": "dummy"}}

# --- Environment Debug Route ---
@app.get("/env-debug")
def env_debug():
    import os
    return {"GOOGLE_GEMINI_API_KEY": os.getenv("GOOGLE_GEMINI_API_KEY")}

# --- Health Check Route ---
@app.get(
    "/health",
    tags=["Utilities"],
    summary="Health check",
    response_description="API health status"
)
async def health_check():
    """
    Simple health check endpoint.
    
    Returns basic system information and status.
    """
    return {
        "status": "healthy",
        "version": "1.0.0",
        "timestamp": datetime.now().isoformat(),
        "services": {
            "gemini_client": "ok",
            "file_handler": "ok"
        }
    }

# --- Application Entry Point ---
# --- LLM API Key Debug Logging ---
from utils.gemini_client import GeminiClient
import logging

def log_gemini_key():
    key = os.environ.get("GOOGLE_GEMINI_API_KEY", "")
    masked = key[:4] + "..." + key[-4:] if key and len(key) > 8 else "NOT SET"
    logging.info(f"[Startup] GOOGLE_GEMINI_API_KEY loaded: {masked}")

log_gemini_key()

if __name__ == "__main__":
    port = int(os.environ.get("PORT", 8000))
    host = os.environ.get("HOST", "0.0.0.0")
    
    logger.info(f"Starting LLM Assistant API on {host}:{port}")
    uvicorn.run(
        "main:app", 
        host=host, 
        port=port, 
        reload=True,
        log_level="info"
    )
