{"timestamp": "2025-04-16T18:04:49.909372", "level": "INFO", "message": "Closing LLM client...", "module": "main", "function": "shutdown_event", "line": 110}
{"timestamp": "2025-04-16T18:04:49.915261", "level": "INFO", "message": "LLM client closed.", "module": "main", "function": "shutdown_event", "line": 112}
{"timestamp": "2025-04-16T18:10:54.345606", "level": "INFO", "message": "Closing LLM client...", "module": "main", "function": "shutdown_event", "line": 110}
{"timestamp": "2025-04-16T18:10:54.355371", "level": "INFO", "message": "LLM client closed.", "module": "main", "function": "shutdown_event", "line": 112}
{"timestamp": "2025-04-16T18:14:49.416928", "level": "INFO", "message": "Closing LLM client...", "module": "main", "function": "shutdown_event", "line": 110}
{"timestamp": "2025-04-16T18:14:49.419998", "level": "INFO", "message": "LLM client closed.", "module": "main", "function": "shutdown_event", "line": 112}
{"timestamp": "2025-04-16T18:15:00.890155", "level": "INFO", "message": "Closing LLM client...", "module": "main", "function": "shutdown_event", "line": 111}
{"timestamp": "2025-04-16T18:15:00.897413", "level": "INFO", "message": "LLM client closed.", "module": "main", "function": "shutdown_event", "line": 113}
{"timestamp": "2025-04-16T18:22:41.314734", "level": "INFO", "message": "Request received", "module": "logger", "function": "log_request", "line": 74}
{"timestamp": "2025-04-16T18:22:45.610499", "level": "INFO", "message": "Response sent", "module": "logger", "function": "log_response", "line": 91}
{"timestamp": "2025-04-16T18:27:38.263315", "level": "INFO", "message": "Request received", "module": "logger", "function": "log_request", "line": 74}
{"timestamp": "2025-04-16T18:27:40.046496", "level": "INFO", "message": "Response sent", "module": "logger", "function": "log_response", "line": 91}
{"timestamp": "2025-04-16T18:35:01.560208", "level": "INFO", "message": "Request received", "module": "logger", "function": "log_request", "line": 74}
{"timestamp": "2025-04-16T18:35:01.560502", "level": "INFO", "message": "Processing chat with input: hi", "module": "chat", "function": "process", "line": 15}
{"timestamp": "2025-04-16T18:35:02.598483", "level": "INFO", "message": "Chat response generated via LLMClient.", "module": "chat", "function": "process", "line": 32}
{"timestamp": "2025-04-16T18:35:02.598742", "level": "INFO", "message": "Response sent", "module": "logger", "function": "log_response", "line": 91}
{"timestamp": "2025-04-16T18:36:44.658506", "level": "INFO", "message": "Request received", "module": "logger", "function": "log_request", "line": 74}
{"timestamp": "2025-04-16T18:36:44.658849", "level": "INFO", "message": "Processing chat with input: hi", "module": "chat", "function": "process", "line": 15}
{"timestamp": "2025-04-16T18:36:45.704540", "level": "INFO", "message": "Chat response generated via LLMClient.", "module": "chat", "function": "process", "line": 32}
{"timestamp": "2025-04-16T18:36:45.704682", "level": "INFO", "message": "Response sent", "module": "logger", "function": "log_response", "line": 91}
{"timestamp": "2025-04-16T18:38:17.514583", "level": "INFO", "message": "Request received", "module": "logger", "function": "log_request", "line": 74}
{"timestamp": "2025-04-16T18:38:25.409648", "level": "INFO", "message": "Response sent", "module": "logger", "function": "log_response", "line": 91}
{"timestamp": "2025-04-16T18:41:06.754123", "level": "INFO", "message": "Request received", "module": "logger", "function": "log_request", "line": 74}
{"timestamp": "2025-04-16T18:41:06.754734", "level": "INFO", "message": "Processing chat with input: hi", "module": "chat", "function": "process", "line": 15}
{"timestamp": "2025-04-16T18:41:07.608620", "level": "INFO", "message": "Chat response generated via LLMClient.", "module": "chat", "function": "process", "line": 32}
{"timestamp": "2025-04-16T18:41:07.608883", "level": "INFO", "message": "Response sent", "module": "logger", "function": "log_response", "line": 91}
{"timestamp": "2025-04-16T18:43:40.524713", "level": "INFO", "message": "Request received", "module": "logger", "function": "log_request", "line": 74}
{"timestamp": "2025-04-16T18:43:40.525583", "level": "INFO", "message": "Processing chat with input: hi", "module": "chat", "function": "process", "line": 15}
{"timestamp": "2025-04-16T18:43:42.655101", "level": "INFO", "message": "Chat response generated via LLMClient.", "module": "chat", "function": "process", "line": 32}
{"timestamp": "2025-04-16T18:43:42.655912", "level": "INFO", "message": "Response sent", "module": "logger", "function": "log_response", "line": 91}
{"timestamp": "2025-04-16T18:55:35.855225", "level": "INFO", "message": "Request received", "module": "logger", "function": "log_request", "line": 74}
{"timestamp": "2025-04-16T18:55:35.861626", "level": "ERROR", "message": "Error during LLMClient summarization: 'LLMClient' object has no attribute 'model_name'", "module": "summarize", "function": "process", "line": 86, "exception": "Traceback (most recent call last):\n  File \"/Users/simba/Downloads/Personal AI Agent/services/summarize.py\", line 47, in process\n    model = parameters.get(\"model\", self.llm_client.model_name) if parameters else self.llm_client.model_name\nAttributeError: 'LLMClient' object has no attribute 'model_name'"}
{"timestamp": "2025-04-16T18:55:35.862051", "level": "INFO", "message": "Response sent", "module": "logger", "function": "log_response", "line": 91}
{"timestamp": "2025-04-16T18:56:01.676265", "level": "INFO", "message": "Request received", "module": "logger", "function": "log_request", "line": 74}
{"timestamp": "2025-04-16T18:56:01.676568", "level": "INFO", "message": "Processing chat with input: hi", "module": "chat", "function": "process", "line": 15}
{"timestamp": "2025-04-16T18:56:03.062611", "level": "INFO", "message": "Chat response generated via LLMClient.", "module": "chat", "function": "process", "line": 32}
{"timestamp": "2025-04-16T18:56:03.063082", "level": "INFO", "message": "Response sent", "module": "logger", "function": "log_response", "line": 91}
{"timestamp": "2025-04-16T18:56:29.038814", "level": "INFO", "message": "Request received", "module": "logger", "function": "log_request", "line": 74}
{"timestamp": "2025-04-16T18:56:37.879683", "level": "INFO", "message": "Response sent", "module": "logger", "function": "log_response", "line": 91}
{"timestamp": "2025-04-16T18:56:55.444137", "level": "INFO", "message": "Request received", "module": "logger", "function": "log_request", "line": 74}
{"timestamp": "2025-04-16T18:56:58.525331", "level": "INFO", "message": "Response sent", "module": "logger", "function": "log_response", "line": 91}
{"timestamp": "2025-04-16T18:57:10.002423", "level": "INFO", "message": "Request received", "module": "logger", "function": "log_request", "line": 74}
{"timestamp": "2025-04-16T18:57:19.817974", "level": "INFO", "message": "Response sent", "module": "logger", "function": "log_response", "line": 91}
{"timestamp": "2025-04-16T19:04:15.030101", "level": "INFO", "message": "Closing LLM client...", "module": "main", "function": "shutdown_event", "line": 111}
{"timestamp": "2025-04-16T19:04:15.030627", "level": "INFO", "message": "LLM client closed.", "module": "main", "function": "shutdown_event", "line": 113}
{"timestamp": "2025-04-16T19:04:16.307890", "level": "INFO", "message": "Closing LLM client...", "module": "main", "function": "shutdown_event", "line": 111}
{"timestamp": "2025-04-16T19:04:16.311444", "level": "INFO", "message": "LLM client closed.", "module": "main", "function": "shutdown_event", "line": 113}
{"timestamp": "2025-04-16T19:20:00.014475", "level": "INFO", "message": "Request received", "module": "logger", "function": "log_request", "line": 74}
{"timestamp": "2025-04-16T19:20:04.239015", "level": "INFO", "message": "Response sent", "module": "logger", "function": "log_response", "line": 91}
{"timestamp": "2025-04-16T19:20:46.463956", "level": "INFO", "message": "Request received", "module": "logger", "function": "log_request", "line": 74}
{"timestamp": "2025-04-16T19:20:50.411465", "level": "INFO", "message": "Response sent", "module": "logger", "function": "log_response", "line": 91}
{"timestamp": "2025-04-16T20:25:50.778088", "level": "INFO", "message": "Closing LLM client...", "module": "main", "function": "shutdown_event", "line": 111}
{"timestamp": "2025-04-16T20:25:50.778789", "level": "INFO", "message": "LLM client closed.", "module": "main", "function": "shutdown_event", "line": 113}
{"timestamp": "2025-04-16T20:26:31.489583", "level": "INFO", "message": "Request received", "module": "logger", "function": "log_request", "line": 74}
{"timestamp": "2025-04-16T20:26:39.584941", "level": "INFO", "message": "Response sent", "module": "logger", "function": "log_response", "line": 91}
{"timestamp": "2025-04-16T20:27:03.843335", "level": "INFO", "message": "Request received", "module": "logger", "function": "log_request", "line": 74}
{"timestamp": "2025-04-16T20:27:06.548827", "level": "INFO", "message": "Response sent", "module": "logger", "function": "log_response", "line": 91}
{"timestamp": "2025-04-18T21:00:39.679269", "level": "INFO", "message": "Request received", "module": "logger", "function": "log_request", "line": 74}
{"timestamp": "2025-04-18T21:00:49.889457", "level": "INFO", "message": "Response sent", "module": "logger", "function": "log_response", "line": 91}
{"timestamp": "2025-04-18T21:01:00.334101", "level": "INFO", "message": "Request received", "module": "logger", "function": "log_request", "line": 74}
{"timestamp": "2025-04-18T21:01:00.334236", "level": "INFO", "message": "Processing chat with input: hello", "module": "chat", "function": "process", "line": 15}
{"timestamp": "2025-04-18T21:01:01.352278", "level": "INFO", "message": "Chat response generated via LLMClient.", "module": "chat", "function": "process", "line": 32}
{"timestamp": "2025-04-18T21:01:01.352343", "level": "INFO", "message": "Response sent", "module": "logger", "function": "log_response", "line": 91}
{"timestamp": "2025-04-18T21:01:11.597685", "level": "INFO", "message": "Request received", "module": "logger", "function": "log_request", "line": 74}
{"timestamp": "2025-04-18T21:01:11.597856", "level": "INFO", "message": "Processing chat with input: 6 times 52", "module": "chat", "function": "process", "line": 15}
{"timestamp": "2025-04-18T21:01:12.648473", "level": "INFO", "message": "Chat response generated via LLMClient.", "module": "chat", "function": "process", "line": 32}
{"timestamp": "2025-04-18T21:01:12.648546", "level": "INFO", "message": "Response sent", "module": "logger", "function": "log_response", "line": 91}
{"timestamp": "2025-04-18T21:03:25.399347", "level": "INFO", "message": "Closing LLM client...", "module": "main", "function": "shutdown_event", "line": 111}
{"timestamp": "2025-04-18T21:03:25.400033", "level": "INFO", "message": "LLM client closed.", "module": "main", "function": "shutdown_event", "line": 113}
{"timestamp": "2025-04-18T21:20:31.078540", "level": "INFO", "message": "Closing LLM client...", "module": "main", "function": "shutdown_event", "line": 111}
{"timestamp": "2025-04-18T21:20:31.083847", "level": "INFO", "message": "LLM client closed.", "module": "main", "function": "shutdown_event", "line": 113}
{"timestamp": "2025-04-18T21:20:47.909607", "level": "INFO", "message": "Request received", "module": "logger", "function": "log_request", "line": 74}
{"timestamp": "2025-04-18T21:20:47.909684", "level": "INFO", "message": "Response sent", "module": "logger", "function": "log_response", "line": 91}
{"timestamp": "2025-04-18T21:20:58.830774", "level": "INFO", "message": "Request received", "module": "logger", "function": "log_request", "line": 74}
{"timestamp": "2025-04-18T21:20:58.830930", "level": "INFO", "message": "Response sent", "module": "logger", "function": "log_response", "line": 91}
{"timestamp": "2025-04-18T21:21:04.986910", "level": "INFO", "message": "Request received", "module": "logger", "function": "log_request", "line": 74}
{"timestamp": "2025-04-18T21:21:04.987071", "level": "INFO", "message": "Response sent", "module": "logger", "function": "log_response", "line": 91}
{"timestamp": "2025-04-18T21:21:19.747107", "level": "INFO", "message": "Request received", "module": "logger", "function": "log_request", "line": 74}
{"timestamp": "2025-04-18T21:21:26.688252", "level": "INFO", "message": "Response sent", "module": "logger", "function": "log_response", "line": 91}
{"timestamp": "2025-04-18T21:22:12.488647", "level": "INFO", "message": "Closing LLM client...", "module": "main", "function": "shutdown_event", "line": 111}
{"timestamp": "2025-04-18T21:22:12.488953", "level": "INFO", "message": "LLM client closed.", "module": "main", "function": "shutdown_event", "line": 113}
{"timestamp": "2025-04-18T21:22:22.643084", "level": "INFO", "message": "Closing LLM client...", "module": "main", "function": "shutdown_event", "line": 111}
{"timestamp": "2025-04-18T21:22:22.647689", "level": "INFO", "message": "LLM client closed.", "module": "main", "function": "shutdown_event", "line": 113}
{"timestamp": "2025-04-18T21:22:29.787375", "level": "INFO", "message": "Request received", "module": "logger", "function": "log_request", "line": 74}
{"timestamp": "2025-04-18T21:22:29.787467", "level": "INFO", "message": "Response sent", "module": "logger", "function": "log_response", "line": 91}
{"timestamp": "2025-04-18T21:22:35.751449", "level": "INFO", "message": "Request received", "module": "logger", "function": "log_request", "line": 74}
{"timestamp": "2025-04-18T21:22:44.292815", "level": "INFO", "message": "Response sent", "module": "logger", "function": "log_response", "line": 91}
{"timestamp": "2025-04-18T21:24:10.992047", "level": "INFO", "message": "Request received", "module": "logger", "function": "log_request", "line": 74}
{"timestamp": "2025-04-18T21:24:10.992318", "level": "INFO", "message": "Response sent", "module": "logger", "function": "log_response", "line": 91}
{"timestamp": "2025-04-18T21:24:19.538127", "level": "INFO", "message": "Request received", "module": "logger", "function": "log_request", "line": 74}
{"timestamp": "2025-04-18T21:24:22.190577", "level": "INFO", "message": "Response sent", "module": "logger", "function": "log_response", "line": 91}
{"timestamp": "2025-04-18T21:26:07.242914", "level": "INFO", "message": "Request received", "module": "logger", "function": "log_request", "line": 74}
{"timestamp": "2025-04-18T21:26:07.243129", "level": "INFO", "message": "Processing chat with input: odd numbers  in python", "module": "chat", "function": "process", "line": 15}
{"timestamp": "2025-04-18T21:26:09.607949", "level": "INFO", "message": "Chat response generated via LLMClient.", "module": "chat", "function": "process", "line": 32}
{"timestamp": "2025-04-18T21:26:09.608561", "level": "INFO", "message": "Response sent", "module": "logger", "function": "log_response", "line": 91}
{"timestamp": "2025-04-18T21:27:31.971469", "level": "INFO", "message": "Closing LLM client...", "module": "main", "function": "shutdown_event", "line": 111}
{"timestamp": "2025-04-18T21:27:31.973045", "level": "INFO", "message": "LLM client closed.", "module": "main", "function": "shutdown_event", "line": 113}
{"timestamp": "2025-04-18T21:27:39.569142", "level": "INFO", "message": "Closing LLM client...", "module": "main", "function": "shutdown_event", "line": 111}
{"timestamp": "2025-04-18T21:27:39.571371", "level": "INFO", "message": "LLM client closed.", "module": "main", "function": "shutdown_event", "line": 113}
{"timestamp": "2025-04-18T21:27:56.289627", "level": "INFO", "message": "Request received", "module": "logger", "function": "log_request", "line": 74}
{"timestamp": "2025-04-18T21:27:56.289711", "level": "INFO", "message": "Processing chat with input: even numbers in python", "module": "chat", "function": "process", "line": 15}
{"timestamp": "2025-04-18T21:28:00.043678", "level": "INFO", "message": "Chat response generated via LLMClient.", "module": "chat", "function": "process", "line": 32}
{"timestamp": "2025-04-18T21:28:00.043926", "level": "INFO", "message": "Response sent", "module": "logger", "function": "log_response", "line": 91}
{"timestamp": "2025-04-18T21:31:06.257221", "level": "INFO", "message": "Request received", "module": "logger", "function": "log_request", "line": 74}
{"timestamp": "2025-04-18T21:31:10.731153", "level": "INFO", "message": "Response sent", "module": "logger", "function": "log_response", "line": 91}
{"timestamp": "2025-04-18T21:31:49.132199", "level": "INFO", "message": "Closing LLM client...", "module": "main", "function": "shutdown_event", "line": 111}
{"timestamp": "2025-04-18T21:31:49.133156", "level": "INFO", "message": "LLM client closed.", "module": "main", "function": "shutdown_event", "line": 113}
{"timestamp": "2025-04-18T21:35:21.647776", "level": "INFO", "message": "Closing LLM client...", "module": "main", "function": "shutdown_event", "line": 111}
{"timestamp": "2025-04-18T21:35:21.656120", "level": "INFO", "message": "LLM client closed.", "module": "main", "function": "shutdown_event", "line": 113}
{"timestamp": "2025-04-18T21:36:18.498747", "level": "INFO", "message": "Closing LLM client...", "module": "main", "function": "shutdown_event", "line": 112}
{"timestamp": "2025-04-18T21:36:18.502637", "level": "INFO", "message": "LLM client closed.", "module": "main", "function": "shutdown_event", "line": 114}
{"timestamp": "2025-04-18T21:38:03.561364", "level": "INFO", "message": "Closing LLM client...", "module": "main", "function": "shutdown_event", "line": 112}
{"timestamp": "2025-04-18T21:38:03.562060", "level": "INFO", "message": "LLM client closed.", "module": "main", "function": "shutdown_event", "line": 114}
{"timestamp": "2025-04-18T21:40:22.515275", "level": "INFO", "message": "Request received", "module": "logger", "function": "log_request", "line": 74}
{"timestamp": "2025-04-18T21:40:22.515350", "level": "INFO", "message": "Processing chat with input: hi", "module": "chat", "function": "process", "line": 15}
{"timestamp": "2025-04-18T21:40:23.527857", "level": "INFO", "message": "Chat response generated via LLMClient.", "module": "chat", "function": "process", "line": 32}
{"timestamp": "2025-04-18T21:40:23.527932", "level": "INFO", "message": "Response sent", "module": "logger", "function": "log_response", "line": 91}
{"timestamp": "2025-04-18T21:40:30.111736", "level": "INFO", "message": "Request received", "module": "logger", "function": "log_request", "line": 74}
{"timestamp": "2025-04-18T21:40:30.111974", "level": "INFO", "message": "Response sent", "module": "logger", "function": "log_response", "line": 91}
{"timestamp": "2025-04-18T21:40:42.937131", "level": "INFO", "message": "Request received", "module": "logger", "function": "log_request", "line": 74}
{"timestamp": "2025-04-18T21:40:50.463251", "level": "INFO", "message": "Response sent", "module": "logger", "function": "log_response", "line": 91}
{"timestamp": "2025-04-18T21:41:05.290189", "level": "INFO", "message": "Request received", "module": "logger", "function": "log_request", "line": 74}
{"timestamp": "2025-04-18T21:41:07.661178", "level": "INFO", "message": "Response sent", "module": "logger", "function": "log_response", "line": 91}
{"timestamp": "2025-04-18T22:15:49.649451", "level": "INFO", "message": "Closing LLM client...", "module": "main", "function": "shutdown_event", "line": 112}
{"timestamp": "2025-04-18T22:15:49.650228", "level": "INFO", "message": "LLM client closed.", "module": "main", "function": "shutdown_event", "line": 114}
{"timestamp": "2025-04-18T22:16:24.022472", "level": "INFO", "message": "Request received", "module": "logger", "function": "log_request", "line": 74}
{"timestamp": "2025-04-18T22:16:28.513581", "level": "INFO", "message": "Response sent", "module": "logger", "function": "log_response", "line": 91}
{"timestamp": "2025-04-18T22:17:32.677041", "level": "INFO", "message": "Closing LLM client...", "module": "main", "function": "shutdown_event", "line": 112}
{"timestamp": "2025-04-18T22:17:32.677302", "level": "INFO", "message": "LLM client closed.", "module": "main", "function": "shutdown_event", "line": 114}
{"timestamp": "2025-04-18T22:17:34.624837", "level": "INFO", "message": "Closing LLM client...", "module": "main", "function": "shutdown_event", "line": 112}
{"timestamp": "2025-04-18T22:17:34.629546", "level": "INFO", "message": "LLM client closed.", "module": "main", "function": "shutdown_event", "line": 114}
{"timestamp": "2025-04-18T22:21:53.120074", "level": "INFO", "message": "Request received", "module": "logger", "function": "log_request", "line": 74}
{"timestamp": "2025-04-18T22:21:53.120153", "level": "INFO", "message": "Response sent", "module": "logger", "function": "log_response", "line": 91}
{"timestamp": "2025-04-18T22:22:00.474858", "level": "INFO", "message": "Request received", "module": "logger", "function": "log_request", "line": 74}
{"timestamp": "2025-04-18T22:22:07.551407", "level": "INFO", "message": "Response sent", "module": "logger", "function": "log_response", "line": 91}
{"timestamp": "2025-04-18T22:23:13.092411", "level": "INFO", "message": "Request received", "module": "logger", "function": "log_request", "line": 74}
{"timestamp": "2025-04-18T22:23:16.888077", "level": "INFO", "message": "Response sent", "module": "logger", "function": "log_response", "line": 91}
{"timestamp": "2025-04-18T22:23:37.391551", "level": "INFO", "message": "Request received", "module": "logger", "function": "log_request", "line": 74}
{"timestamp": "2025-04-18T22:23:39.161326", "level": "INFO", "message": "Response sent", "module": "logger", "function": "log_response", "line": 91}
{"timestamp": "2025-04-18T22:23:57.534554", "level": "INFO", "message": "Request received", "module": "logger", "function": "log_request", "line": 74}
{"timestamp": "2025-04-18T22:24:06.992306", "level": "INFO", "message": "Response sent", "module": "logger", "function": "log_response", "line": 91}
{"timestamp": "2025-04-18T22:56:59.692717", "level": "INFO", "message": "Closing LLM client...", "module": "main", "function": "shutdown_event", "line": 112}
{"timestamp": "2025-04-18T22:56:59.693700", "level": "INFO", "message": "LLM client closed.", "module": "main", "function": "shutdown_event", "line": 114}
{"timestamp": "2025-04-21T17:07:03.488648", "level": "INFO", "message": "Closing LLM client...", "module": "main", "function": "shutdown_event", "line": 112}
{"timestamp": "2025-04-21T17:07:03.497703", "level": "INFO", "message": "LLM client closed.", "module": "main", "function": "shutdown_event", "line": 114}
{"timestamp": "2025-04-21T17:09:31.856296", "level": "INFO", "message": "Closing LLM client...", "module": "main", "function": "shutdown_event", "line": 112}
{"timestamp": "2025-04-21T17:09:31.866458", "level": "INFO", "message": "LLM client closed.", "module": "main", "function": "shutdown_event", "line": 114}
{"timestamp": "2025-04-21T17:13:00.837387", "level": "INFO", "message": "Closing LLM client...", "module": "main", "function": "shutdown_event", "line": 112}
{"timestamp": "2025-04-21T17:13:00.838447", "level": "INFO", "message": "LLM client closed.", "module": "main", "function": "shutdown_event", "line": 114}
{"timestamp": "2025-04-21T17:18:45.928080", "level": "INFO", "message": "Closing LLM client...", "module": "main", "function": "shutdown_event", "line": 112}
{"timestamp": "2025-04-21T17:18:45.936161", "level": "INFO", "message": "LLM client closed.", "module": "main", "function": "shutdown_event", "line": 114}
{"timestamp": "2025-04-21T17:38:11.366541", "level": "INFO", "message": "Closing LLM client...", "module": "main", "function": "shutdown_event", "line": 112}
{"timestamp": "2025-04-21T17:38:11.375726", "level": "INFO", "message": "LLM client closed.", "module": "main", "function": "shutdown_event", "line": 114}
{"timestamp": "2025-04-21T17:41:11.675875", "level": "INFO", "message": "Closing LLM client...", "module": "main", "function": "shutdown_event", "line": 112}
{"timestamp": "2025-04-21T17:41:11.676642", "level": "INFO", "message": "LLM client closed.", "module": "main", "function": "shutdown_event", "line": 114}
{"timestamp": "2025-04-21T17:49:35.867660", "level": "INFO", "message": "Closing LLM client...", "module": "main", "function": "shutdown_event", "line": 112}
{"timestamp": "2025-04-21T17:49:35.867905", "level": "INFO", "message": "LLM client closed.", "module": "main", "function": "shutdown_event", "line": 114}
{"timestamp": "2025-04-21T17:53:01.913233", "level": "INFO", "message": "Closing LLM client...", "module": "main", "function": "shutdown_event", "line": 112}
{"timestamp": "2025-04-21T17:53:01.917509", "level": "INFO", "message": "LLM client closed.", "module": "main", "function": "shutdown_event", "line": 114}
{"timestamp": "2025-04-21T19:39:04.809934", "level": "INFO", "message": "Closing LLM client...", "module": "main", "function": "shutdown_event", "line": 112}
{"timestamp": "2025-04-21T19:39:04.819090", "level": "INFO", "message": "LLM client closed.", "module": "main", "function": "shutdown_event", "line": 114}
{"timestamp": "2025-04-21T20:40:41.850492", "level": "INFO", "message": "Closing LLM client...", "module": "main", "function": "shutdown_event", "line": 112}
{"timestamp": "2025-04-21T20:40:41.854909", "level": "INFO", "message": "LLM client closed.", "module": "main", "function": "shutdown_event", "line": 114}
{"timestamp": "2025-04-21T20:41:50.670668", "level": "INFO", "message": "Closing LLM client...", "module": "main", "function": "shutdown_event", "line": 112}
{"timestamp": "2025-04-21T20:41:50.672466", "level": "INFO", "message": "LLM client closed.", "module": "main", "function": "shutdown_event", "line": 114}
{"timestamp": "2025-04-21T21:22:52.290212", "level": "INFO", "message": "Request received", "module": "logger", "function": "log_request", "line": 74}
{"timestamp": "2025-04-21T21:22:52.291203", "level": "INFO", "message": "Processing chat with input: hello", "module": "chat", "function": "process", "line": 15}
{"timestamp": "2025-04-21T21:23:02.754878", "level": "ERROR", "message": "Error during LLMClient chat processing: RetryError[<Future at 0x1118617c0 state=finished raised Exception>]", "module": "chat", "function": "process", "line": 48, "exception": "Traceback (most recent call last):\n  File \"/Users/simba/Downloads/Personal AI Agent/utils/llm_client.py\", line 57, in generate_response\n    \"content\": data[\"choices\"][0][\"message\"][\"content\"],\nKeyError: 'choices'\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/Users/simba/Downloads/Personal AI Agent/utils/llm_client.py\", line 76, in generate_response\n    local_response.raise_for_status()\n  File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/httpx/_models.py\", line 829, in raise_for_status\n    raise HTTPStatusError(message, request=request, response=self)\nhttpx.HTTPStatusError: Client error '404 Not Found' for url 'http://localhost:11434/api/generate'\nFor more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/404\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/tenacity/asyncio/__init__.py\", line 114, in __call__\n    result = await fn(*args, **kwargs)\n  File \"/Users/simba/Downloads/Personal AI Agent/utils/llm_client.py\", line 85, in generate_response\n    raise Exception(\"Both OpenRouter and local fallback failed.\")\nException: Both OpenRouter and local fallback failed.\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/Users/simba/Downloads/Personal AI Agent/services/chat.py\", line 24, in process\n    response = await self.llm_client.generate_response(\n  File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/tenacity/asyncio/__init__.py\", line 189, in async_wrapped\n    return await copy(fn, *args, **kwargs)\n  File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/tenacity/asyncio/__init__.py\", line 111, in __call__\n    do = await self.iter(retry_state=retry_state)\n  File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n  File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n  File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/tenacity/__init__.py\", line 421, in exc_check\n    raise retry_exc from fut.exception()\ntenacity.RetryError: RetryError[<Future at 0x1118617c0 state=finished raised Exception>]"}
{"timestamp": "2025-04-21T21:23:02.755231", "level": "INFO", "message": "Response sent", "module": "logger", "function": "log_response", "line": 91}
{"timestamp": "2025-04-21T21:43:25.374428", "level": "INFO", "message": "Closing LLM client...", "module": "main", "function": "shutdown_event", "line": 112}
{"timestamp": "2025-04-21T21:43:25.376721", "level": "INFO", "message": "LLM client closed.", "module": "main", "function": "shutdown_event", "line": 114}
{"timestamp": "2025-04-21T21:51:25.169318", "level": "INFO", "message": "Request received", "module": "logger", "function": "log_request", "line": 74}
{"timestamp": "2025-04-21T21:51:25.169402", "level": "INFO", "message": "Processing chat with input: hello", "module": "chat", "function": "process", "line": 15}
{"timestamp": "2025-04-21T21:51:38.373117", "level": "ERROR", "message": "Error during LLMClient chat processing: RetryError[<Future at 0x1055f4e50 state=finished raised Exception>]", "module": "chat", "function": "process", "line": 48, "exception": "Traceback (most recent call last):\n  File \"/Users/simba/Downloads/Personal AI Agent/utils/llm_client.py\", line 57, in generate_response\n    \"content\": data[\"choices\"][0][\"message\"][\"content\"],\nKeyError: 'choices'\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/Users/simba/Downloads/Personal AI Agent/utils/llm_client.py\", line 76, in generate_response\n    local_response.raise_for_status()\n  File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/httpx/_models.py\", line 829, in raise_for_status\n    raise HTTPStatusError(message, request=request, response=self)\nhttpx.HTTPStatusError: Client error '404 Not Found' for url 'http://localhost:11434/api/generate'\nFor more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/404\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/tenacity/asyncio/__init__.py\", line 114, in __call__\n    result = await fn(*args, **kwargs)\n  File \"/Users/simba/Downloads/Personal AI Agent/utils/llm_client.py\", line 85, in generate_response\n    raise Exception(\"Both OpenRouter and local fallback failed.\")\nException: Both OpenRouter and local fallback failed.\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/Users/simba/Downloads/Personal AI Agent/services/chat.py\", line 24, in process\n    response = await self.llm_client.generate_response(\n  File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/tenacity/asyncio/__init__.py\", line 189, in async_wrapped\n    return await copy(fn, *args, **kwargs)\n  File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/tenacity/asyncio/__init__.py\", line 111, in __call__\n    do = await self.iter(retry_state=retry_state)\n  File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/tenacity/asyncio/__init__.py\", line 153, in iter\n    result = await action(retry_state)\n  File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/tenacity/_utils.py\", line 99, in inner\n    return call(*args, **kwargs)\n  File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/tenacity/__init__.py\", line 421, in exc_check\n    raise retry_exc from fut.exception()\ntenacity.RetryError: RetryError[<Future at 0x1055f4e50 state=finished raised Exception>]"}
{"timestamp": "2025-04-21T21:51:38.373541", "level": "INFO", "message": "Response sent", "module": "logger", "function": "log_response", "line": 91}
{"timestamp": "2025-04-21T22:14:30.841923", "level": "INFO", "message": "Closing LLM client...", "module": "main", "function": "shutdown_event", "line": 112}
{"timestamp": "2025-04-21T22:14:30.843263", "level": "INFO", "message": "LLM client closed.", "module": "main", "function": "shutdown_event", "line": 114}
{"timestamp": "2025-04-22T02:35:44.733695", "level": "INFO", "message": "Starting LLM Assistant API", "module": "main", "function": "<module>", "line": 249}
{"timestamp": "2025-04-22T03:09:04.542013", "level": "INFO", "message": "Request received", "module": "logger", "function": "log_request", "line": 74}
{"timestamp": "2025-04-22T03:09:04.542942", "level": "INFO", "message": "Processing chat with input: hi", "module": "chat", "function": "process", "line": 15}
{"timestamp": "2025-04-22T03:09:05.709629", "level": "INFO", "message": "Chat response generated via LLMClient.", "module": "chat", "function": "process", "line": 32}
{"timestamp": "2025-04-22T03:09:05.709762", "level": "INFO", "message": "Response sent", "module": "logger", "function": "log_response", "line": 91}
{"timestamp": "2025-04-22T03:09:23.473417", "level": "INFO", "message": "Request received", "module": "logger", "function": "log_request", "line": 74}
{"timestamp": "2025-04-22T03:09:27.898757", "level": "INFO", "message": "Response sent", "module": "logger", "function": "log_response", "line": 91}
{"timestamp": "2025-04-22T03:10:05.743443", "level": "INFO", "message": "Request received", "module": "logger", "function": "log_request", "line": 74}
{"timestamp": "2025-04-22T03:10:14.391769", "level": "INFO", "message": "Response sent", "module": "logger", "function": "log_response", "line": 91}
{"timestamp": "2025-04-22T03:10:29.707910", "level": "INFO", "message": "Request received", "module": "logger", "function": "log_request", "line": 74}
{"timestamp": "2025-04-22T03:10:31.902124", "level": "INFO", "message": "Response sent", "module": "logger", "function": "log_response", "line": 91}
{"timestamp": "2025-04-22T03:10:41.890942", "level": "INFO", "message": "Request received", "module": "logger", "function": "log_request", "line": 74}
{"timestamp": "2025-04-22T03:10:42.422069", "level": "INFO", "message": "Response sent", "module": "logger", "function": "log_response", "line": 91}
{"timestamp": "2025-04-22T03:11:29.827855", "level": "INFO", "message": "Request received", "module": "logger", "function": "log_request", "line": 74}
{"timestamp": "2025-04-22T03:11:30.402333", "level": "INFO", "message": "Response sent", "module": "logger", "function": "log_response", "line": 91}
{"timestamp": "2025-04-22T03:11:40.931302", "level": "INFO", "message": "Request received", "module": "logger", "function": "log_request", "line": 74}
{"timestamp": "2025-04-22T03:11:41.888035", "level": "INFO", "message": "Response sent", "module": "logger", "function": "log_response", "line": 91}
{"timestamp": "2025-04-22T04:32:08.286385", "level": "INFO", "message": "Closing LLM client...", "module": "main", "function": "shutdown_event", "line": 112}
{"timestamp": "2025-04-22T04:32:08.287936", "level": "INFO", "message": "LLM client closed.", "module": "main", "function": "shutdown_event", "line": 114}
{"timestamp": "2025-04-22T04:32:09.361581", "level": "INFO", "message": "\ud83d\ude80 Starting LLM Assistant API", "module": "main", "function": "startup_event", "line": 222}
{"timestamp": "2025-04-22T04:32:41.831739", "level": "INFO", "message": "Closing LLM client...", "module": "main", "function": "shutdown_event", "line": 228}
{"timestamp": "2025-04-22T04:32:41.835050", "level": "INFO", "message": "LLM client closed. Application shutting down.", "module": "main", "function": "shutdown_event", "line": 230}
{"timestamp": "2025-04-22T04:32:42.859696", "level": "INFO", "message": "\ud83d\ude80 Starting LLM Assistant API", "module": "main", "function": "startup_event", "line": 222}
{"timestamp": "2025-04-22T14:52:18.809048", "level": "INFO", "message": "\ud83d\ude80 Starting LLM Assistant API", "module": "main", "function": "startup_event", "line": 222}
{"timestamp": "2025-04-22T20:42:21.438686", "level": "INFO", "message": "\ud83d\ude80 Starting LLM Assistant API", "module": "main", "function": "startup_event", "line": 222}
{"timestamp": "2025-04-22T20:42:35.205303", "level": "INFO", "message": "Closing LLM client...", "module": "main", "function": "shutdown_event", "line": 228}
{"timestamp": "2025-04-22T20:42:35.208815", "level": "INFO", "message": "LLM client closed. Application shutting down.", "module": "main", "function": "shutdown_event", "line": 230}
{"timestamp": "2025-04-22T20:42:35.899094", "level": "INFO", "message": "\ud83d\ude80 Starting LLM Assistant API", "module": "main", "function": "startup_event", "line": 222}
{"timestamp": "2025-04-22T20:43:13.730539", "level": "INFO", "message": "Closing LLM client...", "module": "main", "function": "shutdown_event", "line": 228}
{"timestamp": "2025-04-22T20:43:13.734614", "level": "INFO", "message": "LLM client closed. Application shutting down.", "module": "main", "function": "shutdown_event", "line": 230}
{"timestamp": "2025-04-22T20:43:17.133413", "level": "INFO", "message": "\ud83d\ude80 Starting LLM Assistant API", "module": "main", "function": "startup_event", "line": 222}
{"timestamp": "2025-04-22T20:44:24.327584", "level": "WARNING", "message": "Failed login attempt for user: fgf", "module": "main", "function": "login_for_access_token", "line": 281}
{"timestamp": "2025-04-22T20:44:24.327812", "level": "INFO", "message": "Request: POST /token - Time: 0.0017s", "module": "main", "function": "__call__", "line": 153}
{"timestamp": "2025-04-22T20:47:19.637200", "level": "INFO", "message": "Successful login for user: testuser", "module": "main", "function": "login_for_access_token", "line": 288}
{"timestamp": "2025-04-22T20:47:19.646930", "level": "INFO", "message": "Request: POST /token - Time: 0.2667s", "module": "main", "function": "__call__", "line": 153}
{"timestamp": "2025-04-22T20:47:27.326418", "level": "INFO", "message": "Request received", "module": "logger", "function": "log_request", "line": 74}
{"timestamp": "2025-04-22T20:47:29.229631", "level": "INFO", "message": "Request received", "module": "logger", "function": "log_request", "line": 74}
{"timestamp": "2025-04-22T20:47:29.826082", "level": "INFO", "message": "Request received", "module": "logger", "function": "log_request", "line": 74}
{"timestamp": "2025-04-22T20:47:30.088273", "level": "INFO", "message": "Request received", "module": "logger", "function": "log_request", "line": 74}
{"timestamp": "2025-04-22T20:47:30.309386", "level": "INFO", "message": "Request received", "module": "logger", "function": "log_request", "line": 74}
{"timestamp": "2025-04-22T20:47:30.541202", "level": "INFO", "message": "Request received", "module": "logger", "function": "log_request", "line": 74}
{"timestamp": "2025-04-22T20:47:37.527239", "level": "INFO", "message": "Response sent", "module": "logger", "function": "log_response", "line": 91}
{"timestamp": "2025-04-22T20:47:37.528889", "level": "INFO", "message": "Request: POST /assist - Time: 7.2204s", "module": "main", "function": "__call__", "line": 153}
{"timestamp": "2025-04-22T20:47:37.532058", "level": "INFO", "message": "Request received", "module": "logger", "function": "log_request", "line": 74}
{"timestamp": "2025-04-22T20:47:38.281571", "level": "INFO", "message": "Response sent", "module": "logger", "function": "log_response", "line": 91}
{"timestamp": "2025-04-22T20:47:38.282064", "level": "INFO", "message": "Request: POST /assist - Time: 10.9641s", "module": "main", "function": "__call__", "line": 153}
{"timestamp": "2025-04-22T20:47:38.288609", "level": "INFO", "message": "Request received", "module": "logger", "function": "log_request", "line": 74}
{"timestamp": "2025-04-22T20:47:39.041261", "level": "INFO", "message": "Response sent", "module": "logger", "function": "log_response", "line": 91}
{"timestamp": "2025-04-22T20:47:39.041709", "level": "INFO", "message": "Request: POST /assist - Time: 9.8145s", "module": "main", "function": "__call__", "line": 153}
{"timestamp": "2025-04-22T20:47:39.045676", "level": "INFO", "message": "Request received", "module": "logger", "function": "log_request", "line": 74}
{"timestamp": "2025-04-22T20:47:40.482113", "level": "INFO", "message": "Response sent", "module": "logger", "function": "log_response", "line": 91}
{"timestamp": "2025-04-22T20:47:40.482429", "level": "INFO", "message": "Request: POST /assist - Time: 10.6589s", "module": "main", "function": "__call__", "line": 153}
{"timestamp": "2025-04-22T20:47:41.458405", "level": "INFO", "message": "Response sent", "module": "logger", "function": "log_response", "line": 91}
{"timestamp": "2025-04-22T20:47:41.458780", "level": "INFO", "message": "Request: POST /assist - Time: 11.3729s", "module": "main", "function": "__call__", "line": 153}
{"timestamp": "2025-04-22T20:47:46.264063", "level": "INFO", "message": "Response sent", "module": "logger", "function": "log_response", "line": 91}
{"timestamp": "2025-04-22T20:47:46.264594", "level": "INFO", "message": "Request: POST /assist - Time: 15.7261s", "module": "main", "function": "__call__", "line": 153}
{"timestamp": "2025-04-22T20:47:48.417522", "level": "INFO", "message": "Response sent", "module": "logger", "function": "log_response", "line": 91}
{"timestamp": "2025-04-22T20:47:48.417997", "level": "INFO", "message": "Request: POST /assist - Time: 10.1308s", "module": "main", "function": "__call__", "line": 153}
{"timestamp": "2025-04-22T20:47:49.135272", "level": "INFO", "message": "Response sent", "module": "logger", "function": "log_response", "line": 91}
{"timestamp": "2025-04-22T20:47:49.135806", "level": "INFO", "message": "Request: POST /assist - Time: 10.0915s", "module": "main", "function": "__call__", "line": 153}
{"timestamp": "2025-04-22T20:47:52.280412", "level": "INFO", "message": "Response sent", "module": "logger", "function": "log_response", "line": 91}
{"timestamp": "2025-04-22T20:47:52.280660", "level": "INFO", "message": "Request: POST /assist - Time: 14.7497s", "module": "main", "function": "__call__", "line": 153}
{"timestamp": "2025-04-22T20:51:13.974572", "level": "INFO", "message": "Request received", "module": "logger", "function": "log_request", "line": 74}
{"timestamp": "2025-04-22T20:51:27.560932", "level": "INFO", "message": "Response sent", "module": "logger", "function": "log_response", "line": 91}
{"timestamp": "2025-04-22T20:51:27.561457", "level": "INFO", "message": "Request: POST /assist - Time: 13.5896s", "module": "main", "function": "__call__", "line": 153}
{"timestamp": "2025-04-22T21:01:34.901420", "level": "ERROR", "message": "Error in api", "module": "logger", "function": "log_error", "line": 108, "exception": "  + Exception Group Traceback (most recent call last):\n  |   File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/_utils.py\", line 76, in collapse_excgroups\n  |     yield\n  |   File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/middleware/base.py\", line 181, in __call__\n  |     recv_stream.close()\n  |   File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/anyio/_backends/_asyncio.py\", line 772, in __aexit__\n  |     raise BaseExceptionGroup(\n  | exceptiongroup.ExceptionGroup: unhandled errors in a TaskGroup (1 sub-exception)\n  +-+---------------- 1 ----------------\n    | Traceback (most recent call last):\n    |   File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/middleware/errors.py\", line 165, in __call__\n    |     await self.app(scope, receive, _send)\n    |   File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/middleware/cors.py\", line 93, in __call__\n    |     await self.simple_response(scope, receive, send, request_headers=headers)\n    |   File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/middleware/cors.py\", line 144, in simple_response\n    |     await self.app(scope, receive, send)\n    |   File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/middleware/gzip.py\", line 29, in __call__\n    |     await responder(scope, receive, send)\n    |   File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/middleware/gzip.py\", line 126, in __call__\n    |     await super().__call__(scope, receive, send)\n    |   File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/middleware/gzip.py\", line 46, in __call__\n    |     await self.app(scope, receive, self.send_with_compression)\n    |   File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/middleware/base.py\", line 181, in __call__\n    |     recv_stream.close()\n    |   File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/contextlib.py\", line 135, in __exit__\n    |     self.gen.throw(type, value, traceback)\n    |   File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/_utils.py\", line 82, in collapse_excgroups\n    |     raise exc\n    |   File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/middleware/base.py\", line 178, in __call__\n    |     response = await self.dispatch_func(request, call_next)\n    |   File \"/Users/simba/Downloads/Personal AI Agent/main.py\", line 145, in __call__\n    |     response = await call_next(request)\n    |   File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/middleware/base.py\", line 156, in call_next\n    |     raise app_exc\n    |   File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/middleware/base.py\", line 141, in coro\n    |     await self.app(scope, receive_or_disconnect, send_no_error)\n    |   File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/middleware/exceptions.py\", line 62, in __call__\n    |     await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)\n    |   File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\n    |     raise exc\n    |   File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/_exception_handler.py\", line 42, in wrapped_app\n    |     await app(scope, receive, sender)\n    |   File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/routing.py\", line 714, in __call__\n    |     await self.middleware_stack(scope, receive, send)\n    |   File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/routing.py\", line 734, in app\n    |     await route.handle(scope, receive, send)\n    |   File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/routing.py\", line 288, in handle\n    |     await self.app(scope, receive, send)\n    |   File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/routing.py\", line 76, in app\n    |     await wrap_app_handling_exceptions(app, request)(scope, receive, send)\n    |   File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\n    |     raise exc\n    |   File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/_exception_handler.py\", line 42, in wrapped_app\n    |     await app(scope, receive, sender)\n    |   File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/routing.py\", line 73, in app\n    |     response = await f(request)\n    |   File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/fastapi/routing.py\", line 301, in app\n    |     raw_response = await run_endpoint_function(\n    |   File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/fastapi/routing.py\", line 212, in run_endpoint_function\n    |     return await dependant.call(**values)\n    |   File \"/Users/simba/Downloads/Personal AI Agent/main.py\", line 424, in list_todos\n    |     return todo_service.list_tasks(current_user.username)\n    | TypeError: list_tasks() takes 1 positional argument but 2 were given\n    +------------------------------------\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/middleware/errors.py\", line 165, in __call__\n    await self.app(scope, receive, _send)\n  File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/middleware/cors.py\", line 93, in __call__\n    await self.simple_response(scope, receive, send, request_headers=headers)\n  File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/middleware/cors.py\", line 144, in simple_response\n    await self.app(scope, receive, send)\n  File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/middleware/gzip.py\", line 29, in __call__\n    await responder(scope, receive, send)\n  File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/middleware/gzip.py\", line 126, in __call__\n    await super().__call__(scope, receive, send)\n  File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/middleware/gzip.py\", line 46, in __call__\n    await self.app(scope, receive, self.send_with_compression)\n  File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/middleware/base.py\", line 181, in __call__\n    recv_stream.close()\n  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/contextlib.py\", line 135, in __exit__\n    self.gen.throw(type, value, traceback)\n  File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/_utils.py\", line 82, in collapse_excgroups\n    raise exc\n  File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/middleware/base.py\", line 178, in __call__\n    response = await self.dispatch_func(request, call_next)\n  File \"/Users/simba/Downloads/Personal AI Agent/main.py\", line 145, in __call__\n    response = await call_next(request)\n  File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/middleware/base.py\", line 156, in call_next\n    raise app_exc\n  File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/middleware/base.py\", line 141, in coro\n    await self.app(scope, receive_or_disconnect, send_no_error)\n  File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/middleware/exceptions.py\", line 62, in __call__\n    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)\n  File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\n    raise exc\n  File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/_exception_handler.py\", line 42, in wrapped_app\n    await app(scope, receive, sender)\n  File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/routing.py\", line 714, in __call__\n    await self.middleware_stack(scope, receive, send)\n  File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/routing.py\", line 734, in app\n    await route.handle(scope, receive, send)\n  File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/routing.py\", line 288, in handle\n    await self.app(scope, receive, send)\n  File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/routing.py\", line 76, in app\n    await wrap_app_handling_exceptions(app, request)(scope, receive, send)\n  File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\n    raise exc\n  File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/_exception_handler.py\", line 42, in wrapped_app\n    await app(scope, receive, sender)\n  File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/routing.py\", line 73, in app\n    response = await f(request)\n  File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/fastapi/routing.py\", line 301, in app\n    raw_response = await run_endpoint_function(\n  File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/fastapi/routing.py\", line 212, in run_endpoint_function\n    return await dependant.call(**values)\n  File \"/Users/simba/Downloads/Personal AI Agent/main.py\", line 424, in list_todos\n    return todo_service.list_tasks(current_user.username)\nTypeError: list_tasks() takes 1 positional argument but 2 were given"}
{"timestamp": "2025-04-22T21:01:34.911589", "level": "ERROR", "message": "Error in api", "module": "logger", "function": "log_error", "line": 108, "exception": "  + Exception Group Traceback (most recent call last):\n  |   File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/_utils.py\", line 76, in collapse_excgroups\n  |     yield\n  |   File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/middleware/base.py\", line 181, in __call__\n  |     recv_stream.close()\n  |   File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/anyio/_backends/_asyncio.py\", line 772, in __aexit__\n  |     raise BaseExceptionGroup(\n  | exceptiongroup.ExceptionGroup: unhandled errors in a TaskGroup (1 sub-exception)\n  +-+---------------- 1 ----------------\n    | Traceback (most recent call last):\n    |   File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/middleware/errors.py\", line 165, in __call__\n    |     await self.app(scope, receive, _send)\n    |   File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/middleware/cors.py\", line 93, in __call__\n    |     await self.simple_response(scope, receive, send, request_headers=headers)\n    |   File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/middleware/cors.py\", line 144, in simple_response\n    |     await self.app(scope, receive, send)\n    |   File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/middleware/gzip.py\", line 29, in __call__\n    |     await responder(scope, receive, send)\n    |   File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/middleware/gzip.py\", line 126, in __call__\n    |     await super().__call__(scope, receive, send)\n    |   File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/middleware/gzip.py\", line 46, in __call__\n    |     await self.app(scope, receive, self.send_with_compression)\n    |   File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/middleware/base.py\", line 181, in __call__\n    |     recv_stream.close()\n    |   File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/contextlib.py\", line 135, in __exit__\n    |     self.gen.throw(type, value, traceback)\n    |   File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/_utils.py\", line 82, in collapse_excgroups\n    |     raise exc\n    |   File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/middleware/base.py\", line 178, in __call__\n    |     response = await self.dispatch_func(request, call_next)\n    |   File \"/Users/simba/Downloads/Personal AI Agent/main.py\", line 145, in __call__\n    |     response = await call_next(request)\n    |   File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/middleware/base.py\", line 156, in call_next\n    |     raise app_exc\n    |   File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/middleware/base.py\", line 141, in coro\n    |     await self.app(scope, receive_or_disconnect, send_no_error)\n    |   File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/middleware/exceptions.py\", line 62, in __call__\n    |     await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)\n    |   File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\n    |     raise exc\n    |   File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/_exception_handler.py\", line 42, in wrapped_app\n    |     await app(scope, receive, sender)\n    |   File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/routing.py\", line 714, in __call__\n    |     await self.middleware_stack(scope, receive, send)\n    |   File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/routing.py\", line 734, in app\n    |     await route.handle(scope, receive, send)\n    |   File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/routing.py\", line 288, in handle\n    |     await self.app(scope, receive, send)\n    |   File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/routing.py\", line 76, in app\n    |     await wrap_app_handling_exceptions(app, request)(scope, receive, send)\n    |   File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\n    |     raise exc\n    |   File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/_exception_handler.py\", line 42, in wrapped_app\n    |     await app(scope, receive, sender)\n    |   File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/routing.py\", line 73, in app\n    |     response = await f(request)\n    |   File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/fastapi/routing.py\", line 301, in app\n    |     raw_response = await run_endpoint_function(\n    |   File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/fastapi/routing.py\", line 212, in run_endpoint_function\n    |     return await dependant.call(**values)\n    |   File \"/Users/simba/Downloads/Personal AI Agent/main.py\", line 424, in list_todos\n    |     return todo_service.list_tasks(current_user.username)\n    | TypeError: list_tasks() takes 1 positional argument but 2 were given\n    +------------------------------------\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/middleware/errors.py\", line 165, in __call__\n    await self.app(scope, receive, _send)\n  File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/middleware/cors.py\", line 93, in __call__\n    await self.simple_response(scope, receive, send, request_headers=headers)\n  File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/middleware/cors.py\", line 144, in simple_response\n    await self.app(scope, receive, send)\n  File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/middleware/gzip.py\", line 29, in __call__\n    await responder(scope, receive, send)\n  File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/middleware/gzip.py\", line 126, in __call__\n    await super().__call__(scope, receive, send)\n  File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/middleware/gzip.py\", line 46, in __call__\n    await self.app(scope, receive, self.send_with_compression)\n  File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/middleware/base.py\", line 181, in __call__\n    recv_stream.close()\n  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/contextlib.py\", line 135, in __exit__\n    self.gen.throw(type, value, traceback)\n  File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/_utils.py\", line 82, in collapse_excgroups\n    raise exc\n  File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/middleware/base.py\", line 178, in __call__\n    response = await self.dispatch_func(request, call_next)\n  File \"/Users/simba/Downloads/Personal AI Agent/main.py\", line 145, in __call__\n    response = await call_next(request)\n  File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/middleware/base.py\", line 156, in call_next\n    raise app_exc\n  File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/middleware/base.py\", line 141, in coro\n    await self.app(scope, receive_or_disconnect, send_no_error)\n  File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/middleware/exceptions.py\", line 62, in __call__\n    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)\n  File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\n    raise exc\n  File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/_exception_handler.py\", line 42, in wrapped_app\n    await app(scope, receive, sender)\n  File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/routing.py\", line 714, in __call__\n    await self.middleware_stack(scope, receive, send)\n  File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/routing.py\", line 734, in app\n    await route.handle(scope, receive, send)\n  File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/routing.py\", line 288, in handle\n    await self.app(scope, receive, send)\n  File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/routing.py\", line 76, in app\n    await wrap_app_handling_exceptions(app, request)(scope, receive, send)\n  File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\n    raise exc\n  File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/_exception_handler.py\", line 42, in wrapped_app\n    await app(scope, receive, sender)\n  File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/routing.py\", line 73, in app\n    response = await f(request)\n  File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/fastapi/routing.py\", line 301, in app\n    raw_response = await run_endpoint_function(\n  File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/fastapi/routing.py\", line 212, in run_endpoint_function\n    return await dependant.call(**values)\n  File \"/Users/simba/Downloads/Personal AI Agent/main.py\", line 424, in list_todos\n    return todo_service.list_tasks(current_user.username)\nTypeError: list_tasks() takes 1 positional argument but 2 were given"}
{"timestamp": "2025-04-22T21:01:42.023056", "level": "INFO", "message": "Request received", "module": "logger", "function": "log_request", "line": 74}
{"timestamp": "2025-04-22T21:01:54.578578", "level": "INFO", "message": "Response sent", "module": "logger", "function": "log_response", "line": 91}
{"timestamp": "2025-04-22T21:01:54.579062", "level": "INFO", "message": "Request: POST /assist - Time: 12.5598s", "module": "main", "function": "__call__", "line": 153}
{"timestamp": "2025-04-22T21:02:01.023958", "level": "INFO", "message": "Request received", "module": "logger", "function": "log_request", "line": 74}
{"timestamp": "2025-04-22T21:02:07.608850", "level": "ERROR", "message": "Error in api", "module": "logger", "function": "log_error", "line": 108, "exception": "  + Exception Group Traceback (most recent call last):\n  |   File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/_utils.py\", line 76, in collapse_excgroups\n  |     yield\n  |   File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/middleware/base.py\", line 181, in __call__\n  |     recv_stream.close()\n  |   File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/anyio/_backends/_asyncio.py\", line 772, in __aexit__\n  |     raise BaseExceptionGroup(\n  | exceptiongroup.ExceptionGroup: unhandled errors in a TaskGroup (1 sub-exception)\n  +-+---------------- 1 ----------------\n    | Traceback (most recent call last):\n    |   File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/middleware/errors.py\", line 165, in __call__\n    |     await self.app(scope, receive, _send)\n    |   File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/middleware/cors.py\", line 93, in __call__\n    |     await self.simple_response(scope, receive, send, request_headers=headers)\n    |   File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/middleware/cors.py\", line 144, in simple_response\n    |     await self.app(scope, receive, send)\n    |   File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/middleware/gzip.py\", line 29, in __call__\n    |     await responder(scope, receive, send)\n    |   File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/middleware/gzip.py\", line 126, in __call__\n    |     await super().__call__(scope, receive, send)\n    |   File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/middleware/gzip.py\", line 46, in __call__\n    |     await self.app(scope, receive, self.send_with_compression)\n    |   File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/middleware/base.py\", line 181, in __call__\n    |     recv_stream.close()\n    |   File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/contextlib.py\", line 135, in __exit__\n    |     self.gen.throw(type, value, traceback)\n    |   File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/_utils.py\", line 82, in collapse_excgroups\n    |     raise exc\n    |   File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/middleware/base.py\", line 178, in __call__\n    |     response = await self.dispatch_func(request, call_next)\n    |   File \"/Users/simba/Downloads/Personal AI Agent/main.py\", line 145, in __call__\n    |     response = await call_next(request)\n    |   File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/middleware/base.py\", line 156, in call_next\n    |     raise app_exc\n    |   File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/middleware/base.py\", line 141, in coro\n    |     await self.app(scope, receive_or_disconnect, send_no_error)\n    |   File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/middleware/exceptions.py\", line 62, in __call__\n    |     await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)\n    |   File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\n    |     raise exc\n    |   File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/_exception_handler.py\", line 42, in wrapped_app\n    |     await app(scope, receive, sender)\n    |   File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/routing.py\", line 714, in __call__\n    |     await self.middleware_stack(scope, receive, send)\n    |   File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/routing.py\", line 734, in app\n    |     await route.handle(scope, receive, send)\n    |   File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/routing.py\", line 288, in handle\n    |     await self.app(scope, receive, send)\n    |   File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/routing.py\", line 76, in app\n    |     await wrap_app_handling_exceptions(app, request)(scope, receive, send)\n    |   File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\n    |     raise exc\n    |   File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/_exception_handler.py\", line 42, in wrapped_app\n    |     await app(scope, receive, sender)\n    |   File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/routing.py\", line 73, in app\n    |     response = await f(request)\n    |   File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/fastapi/routing.py\", line 301, in app\n    |     raw_response = await run_endpoint_function(\n    |   File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/fastapi/routing.py\", line 212, in run_endpoint_function\n    |     return await dependant.call(**values)\n    |   File \"/Users/simba/Downloads/Personal AI Agent/main.py\", line 424, in list_todos\n    |     return todo_service.list_tasks(current_user.username)\n    | TypeError: list_tasks() takes 1 positional argument but 2 were given\n    +------------------------------------\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/middleware/errors.py\", line 165, in __call__\n    await self.app(scope, receive, _send)\n  File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/middleware/cors.py\", line 93, in __call__\n    await self.simple_response(scope, receive, send, request_headers=headers)\n  File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/middleware/cors.py\", line 144, in simple_response\n    await self.app(scope, receive, send)\n  File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/middleware/gzip.py\", line 29, in __call__\n    await responder(scope, receive, send)\n  File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/middleware/gzip.py\", line 126, in __call__\n    await super().__call__(scope, receive, send)\n  File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/middleware/gzip.py\", line 46, in __call__\n    await self.app(scope, receive, self.send_with_compression)\n  File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/middleware/base.py\", line 181, in __call__\n    recv_stream.close()\n  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/contextlib.py\", line 135, in __exit__\n    self.gen.throw(type, value, traceback)\n  File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/_utils.py\", line 82, in collapse_excgroups\n    raise exc\n  File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/middleware/base.py\", line 178, in __call__\n    response = await self.dispatch_func(request, call_next)\n  File \"/Users/simba/Downloads/Personal AI Agent/main.py\", line 145, in __call__\n    response = await call_next(request)\n  File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/middleware/base.py\", line 156, in call_next\n    raise app_exc\n  File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/middleware/base.py\", line 141, in coro\n    await self.app(scope, receive_or_disconnect, send_no_error)\n  File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/middleware/exceptions.py\", line 62, in __call__\n    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)\n  File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\n    raise exc\n  File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/_exception_handler.py\", line 42, in wrapped_app\n    await app(scope, receive, sender)\n  File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/routing.py\", line 714, in __call__\n    await self.middleware_stack(scope, receive, send)\n  File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/routing.py\", line 734, in app\n    await route.handle(scope, receive, send)\n  File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/routing.py\", line 288, in handle\n    await self.app(scope, receive, send)\n  File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/routing.py\", line 76, in app\n    await wrap_app_handling_exceptions(app, request)(scope, receive, send)\n  File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\n    raise exc\n  File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/_exception_handler.py\", line 42, in wrapped_app\n    await app(scope, receive, sender)\n  File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/routing.py\", line 73, in app\n    response = await f(request)\n  File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/fastapi/routing.py\", line 301, in app\n    raw_response = await run_endpoint_function(\n  File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/fastapi/routing.py\", line 212, in run_endpoint_function\n    return await dependant.call(**values)\n  File \"/Users/simba/Downloads/Personal AI Agent/main.py\", line 424, in list_todos\n    return todo_service.list_tasks(current_user.username)\nTypeError: list_tasks() takes 1 positional argument but 2 were given"}
{"timestamp": "2025-04-22T21:02:07.618053", "level": "ERROR", "message": "Error in api", "module": "logger", "function": "log_error", "line": 108, "exception": "  + Exception Group Traceback (most recent call last):\n  |   File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/_utils.py\", line 76, in collapse_excgroups\n  |     yield\n  |   File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/middleware/base.py\", line 181, in __call__\n  |     recv_stream.close()\n  |   File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/anyio/_backends/_asyncio.py\", line 772, in __aexit__\n  |     raise BaseExceptionGroup(\n  | exceptiongroup.ExceptionGroup: unhandled errors in a TaskGroup (1 sub-exception)\n  +-+---------------- 1 ----------------\n    | Traceback (most recent call last):\n    |   File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/middleware/errors.py\", line 165, in __call__\n    |     await self.app(scope, receive, _send)\n    |   File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/middleware/cors.py\", line 93, in __call__\n    |     await self.simple_response(scope, receive, send, request_headers=headers)\n    |   File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/middleware/cors.py\", line 144, in simple_response\n    |     await self.app(scope, receive, send)\n    |   File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/middleware/gzip.py\", line 29, in __call__\n    |     await responder(scope, receive, send)\n    |   File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/middleware/gzip.py\", line 126, in __call__\n    |     await super().__call__(scope, receive, send)\n    |   File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/middleware/gzip.py\", line 46, in __call__\n    |     await self.app(scope, receive, self.send_with_compression)\n    |   File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/middleware/base.py\", line 181, in __call__\n    |     recv_stream.close()\n    |   File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/contextlib.py\", line 135, in __exit__\n    |     self.gen.throw(type, value, traceback)\n    |   File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/_utils.py\", line 82, in collapse_excgroups\n    |     raise exc\n    |   File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/middleware/base.py\", line 178, in __call__\n    |     response = await self.dispatch_func(request, call_next)\n    |   File \"/Users/simba/Downloads/Personal AI Agent/main.py\", line 145, in __call__\n    |     response = await call_next(request)\n    |   File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/middleware/base.py\", line 156, in call_next\n    |     raise app_exc\n    |   File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/middleware/base.py\", line 141, in coro\n    |     await self.app(scope, receive_or_disconnect, send_no_error)\n    |   File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/middleware/exceptions.py\", line 62, in __call__\n    |     await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)\n    |   File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\n    |     raise exc\n    |   File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/_exception_handler.py\", line 42, in wrapped_app\n    |     await app(scope, receive, sender)\n    |   File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/routing.py\", line 714, in __call__\n    |     await self.middleware_stack(scope, receive, send)\n    |   File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/routing.py\", line 734, in app\n    |     await route.handle(scope, receive, send)\n    |   File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/routing.py\", line 288, in handle\n    |     await self.app(scope, receive, send)\n    |   File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/routing.py\", line 76, in app\n    |     await wrap_app_handling_exceptions(app, request)(scope, receive, send)\n    |   File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\n    |     raise exc\n    |   File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/_exception_handler.py\", line 42, in wrapped_app\n    |     await app(scope, receive, sender)\n    |   File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/routing.py\", line 73, in app\n    |     response = await f(request)\n    |   File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/fastapi/routing.py\", line 301, in app\n    |     raw_response = await run_endpoint_function(\n    |   File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/fastapi/routing.py\", line 212, in run_endpoint_function\n    |     return await dependant.call(**values)\n    |   File \"/Users/simba/Downloads/Personal AI Agent/main.py\", line 424, in list_todos\n    |     return todo_service.list_tasks(current_user.username)\n    | TypeError: list_tasks() takes 1 positional argument but 2 were given\n    +------------------------------------\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/middleware/errors.py\", line 165, in __call__\n    await self.app(scope, receive, _send)\n  File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/middleware/cors.py\", line 93, in __call__\n    await self.simple_response(scope, receive, send, request_headers=headers)\n  File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/middleware/cors.py\", line 144, in simple_response\n    await self.app(scope, receive, send)\n  File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/middleware/gzip.py\", line 29, in __call__\n    await responder(scope, receive, send)\n  File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/middleware/gzip.py\", line 126, in __call__\n    await super().__call__(scope, receive, send)\n  File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/middleware/gzip.py\", line 46, in __call__\n    await self.app(scope, receive, self.send_with_compression)\n  File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/middleware/base.py\", line 181, in __call__\n    recv_stream.close()\n  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/contextlib.py\", line 135, in __exit__\n    self.gen.throw(type, value, traceback)\n  File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/_utils.py\", line 82, in collapse_excgroups\n    raise exc\n  File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/middleware/base.py\", line 178, in __call__\n    response = await self.dispatch_func(request, call_next)\n  File \"/Users/simba/Downloads/Personal AI Agent/main.py\", line 145, in __call__\n    response = await call_next(request)\n  File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/middleware/base.py\", line 156, in call_next\n    raise app_exc\n  File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/middleware/base.py\", line 141, in coro\n    await self.app(scope, receive_or_disconnect, send_no_error)\n  File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/middleware/exceptions.py\", line 62, in __call__\n    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)\n  File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\n    raise exc\n  File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/_exception_handler.py\", line 42, in wrapped_app\n    await app(scope, receive, sender)\n  File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/routing.py\", line 714, in __call__\n    await self.middleware_stack(scope, receive, send)\n  File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/routing.py\", line 734, in app\n    await route.handle(scope, receive, send)\n  File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/routing.py\", line 288, in handle\n    await self.app(scope, receive, send)\n  File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/routing.py\", line 76, in app\n    await wrap_app_handling_exceptions(app, request)(scope, receive, send)\n  File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\n    raise exc\n  File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/_exception_handler.py\", line 42, in wrapped_app\n    await app(scope, receive, sender)\n  File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/routing.py\", line 73, in app\n    response = await f(request)\n  File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/fastapi/routing.py\", line 301, in app\n    raw_response = await run_endpoint_function(\n  File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/fastapi/routing.py\", line 212, in run_endpoint_function\n    return await dependant.call(**values)\n  File \"/Users/simba/Downloads/Personal AI Agent/main.py\", line 424, in list_todos\n    return todo_service.list_tasks(current_user.username)\nTypeError: list_tasks() takes 1 positional argument but 2 were given"}
{"timestamp": "2025-04-22T21:02:10.063459", "level": "ERROR", "message": "Error in api", "module": "logger", "function": "log_error", "line": 108, "exception": "  + Exception Group Traceback (most recent call last):\n  |   File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/_utils.py\", line 76, in collapse_excgroups\n  |     yield\n  |   File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/middleware/base.py\", line 181, in __call__\n  |     recv_stream.close()\n  |   File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/anyio/_backends/_asyncio.py\", line 772, in __aexit__\n  |     raise BaseExceptionGroup(\n  | exceptiongroup.ExceptionGroup: unhandled errors in a TaskGroup (1 sub-exception)\n  +-+---------------- 1 ----------------\n    | Traceback (most recent call last):\n    |   File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/middleware/errors.py\", line 165, in __call__\n    |     await self.app(scope, receive, _send)\n    |   File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/middleware/cors.py\", line 93, in __call__\n    |     await self.simple_response(scope, receive, send, request_headers=headers)\n    |   File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/middleware/cors.py\", line 144, in simple_response\n    |     await self.app(scope, receive, send)\n    |   File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/middleware/gzip.py\", line 29, in __call__\n    |     await responder(scope, receive, send)\n    |   File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/middleware/gzip.py\", line 126, in __call__\n    |     await super().__call__(scope, receive, send)\n    |   File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/middleware/gzip.py\", line 46, in __call__\n    |     await self.app(scope, receive, self.send_with_compression)\n    |   File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/middleware/base.py\", line 181, in __call__\n    |     recv_stream.close()\n    |   File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/contextlib.py\", line 135, in __exit__\n    |     self.gen.throw(type, value, traceback)\n    |   File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/_utils.py\", line 82, in collapse_excgroups\n    |     raise exc\n    |   File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/middleware/base.py\", line 178, in __call__\n    |     response = await self.dispatch_func(request, call_next)\n    |   File \"/Users/simba/Downloads/Personal AI Agent/main.py\", line 145, in __call__\n    |     response = await call_next(request)\n    |   File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/middleware/base.py\", line 156, in call_next\n    |     raise app_exc\n    |   File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/middleware/base.py\", line 141, in coro\n    |     await self.app(scope, receive_or_disconnect, send_no_error)\n    |   File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/middleware/exceptions.py\", line 62, in __call__\n    |     await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)\n    |   File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\n    |     raise exc\n    |   File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/_exception_handler.py\", line 42, in wrapped_app\n    |     await app(scope, receive, sender)\n    |   File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/routing.py\", line 714, in __call__\n    |     await self.middleware_stack(scope, receive, send)\n    |   File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/routing.py\", line 734, in app\n    |     await route.handle(scope, receive, send)\n    |   File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/routing.py\", line 288, in handle\n    |     await self.app(scope, receive, send)\n    |   File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/routing.py\", line 76, in app\n    |     await wrap_app_handling_exceptions(app, request)(scope, receive, send)\n    |   File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\n    |     raise exc\n    |   File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/_exception_handler.py\", line 42, in wrapped_app\n    |     await app(scope, receive, sender)\n    |   File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/routing.py\", line 73, in app\n    |     response = await f(request)\n    |   File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/fastapi/routing.py\", line 301, in app\n    |     raw_response = await run_endpoint_function(\n    |   File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/fastapi/routing.py\", line 212, in run_endpoint_function\n    |     return await dependant.call(**values)\n    |   File \"/Users/simba/Downloads/Personal AI Agent/main.py\", line 424, in list_todos\n    |     return todo_service.list_tasks(current_user.username)\n    | TypeError: list_tasks() takes 1 positional argument but 2 were given\n    +------------------------------------\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/middleware/errors.py\", line 165, in __call__\n    await self.app(scope, receive, _send)\n  File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/middleware/cors.py\", line 93, in __call__\n    await self.simple_response(scope, receive, send, request_headers=headers)\n  File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/middleware/cors.py\", line 144, in simple_response\n    await self.app(scope, receive, send)\n  File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/middleware/gzip.py\", line 29, in __call__\n    await responder(scope, receive, send)\n  File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/middleware/gzip.py\", line 126, in __call__\n    await super().__call__(scope, receive, send)\n  File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/middleware/gzip.py\", line 46, in __call__\n    await self.app(scope, receive, self.send_with_compression)\n  File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/middleware/base.py\", line 181, in __call__\n    recv_stream.close()\n  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/contextlib.py\", line 135, in __exit__\n    self.gen.throw(type, value, traceback)\n  File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/_utils.py\", line 82, in collapse_excgroups\n    raise exc\n  File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/middleware/base.py\", line 178, in __call__\n    response = await self.dispatch_func(request, call_next)\n  File \"/Users/simba/Downloads/Personal AI Agent/main.py\", line 145, in __call__\n    response = await call_next(request)\n  File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/middleware/base.py\", line 156, in call_next\n    raise app_exc\n  File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/middleware/base.py\", line 141, in coro\n    await self.app(scope, receive_or_disconnect, send_no_error)\n  File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/middleware/exceptions.py\", line 62, in __call__\n    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)\n  File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\n    raise exc\n  File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/_exception_handler.py\", line 42, in wrapped_app\n    await app(scope, receive, sender)\n  File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/routing.py\", line 714, in __call__\n    await self.middleware_stack(scope, receive, send)\n  File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/routing.py\", line 734, in app\n    await route.handle(scope, receive, send)\n  File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/routing.py\", line 288, in handle\n    await self.app(scope, receive, send)\n  File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/routing.py\", line 76, in app\n    await wrap_app_handling_exceptions(app, request)(scope, receive, send)\n  File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\n    raise exc\n  File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/_exception_handler.py\", line 42, in wrapped_app\n    await app(scope, receive, sender)\n  File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/routing.py\", line 73, in app\n    response = await f(request)\n  File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/fastapi/routing.py\", line 301, in app\n    raw_response = await run_endpoint_function(\n  File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/fastapi/routing.py\", line 212, in run_endpoint_function\n    return await dependant.call(**values)\n  File \"/Users/simba/Downloads/Personal AI Agent/main.py\", line 424, in list_todos\n    return todo_service.list_tasks(current_user.username)\nTypeError: list_tasks() takes 1 positional argument but 2 were given"}
{"timestamp": "2025-04-22T21:02:10.072948", "level": "ERROR", "message": "Error in api", "module": "logger", "function": "log_error", "line": 108, "exception": "  + Exception Group Traceback (most recent call last):\n  |   File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/_utils.py\", line 76, in collapse_excgroups\n  |     yield\n  |   File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/middleware/base.py\", line 181, in __call__\n  |     recv_stream.close()\n  |   File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/anyio/_backends/_asyncio.py\", line 772, in __aexit__\n  |     raise BaseExceptionGroup(\n  | exceptiongroup.ExceptionGroup: unhandled errors in a TaskGroup (1 sub-exception)\n  +-+---------------- 1 ----------------\n    | Traceback (most recent call last):\n    |   File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/middleware/errors.py\", line 165, in __call__\n    |     await self.app(scope, receive, _send)\n    |   File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/middleware/cors.py\", line 93, in __call__\n    |     await self.simple_response(scope, receive, send, request_headers=headers)\n    |   File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/middleware/cors.py\", line 144, in simple_response\n    |     await self.app(scope, receive, send)\n    |   File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/middleware/gzip.py\", line 29, in __call__\n    |     await responder(scope, receive, send)\n    |   File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/middleware/gzip.py\", line 126, in __call__\n    |     await super().__call__(scope, receive, send)\n    |   File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/middleware/gzip.py\", line 46, in __call__\n    |     await self.app(scope, receive, self.send_with_compression)\n    |   File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/middleware/base.py\", line 181, in __call__\n    |     recv_stream.close()\n    |   File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/contextlib.py\", line 135, in __exit__\n    |     self.gen.throw(type, value, traceback)\n    |   File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/_utils.py\", line 82, in collapse_excgroups\n    |     raise exc\n    |   File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/middleware/base.py\", line 178, in __call__\n    |     response = await self.dispatch_func(request, call_next)\n    |   File \"/Users/simba/Downloads/Personal AI Agent/main.py\", line 145, in __call__\n    |     response = await call_next(request)\n    |   File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/middleware/base.py\", line 156, in call_next\n    |     raise app_exc\n    |   File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/middleware/base.py\", line 141, in coro\n    |     await self.app(scope, receive_or_disconnect, send_no_error)\n    |   File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/middleware/exceptions.py\", line 62, in __call__\n    |     await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)\n    |   File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\n    |     raise exc\n    |   File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/_exception_handler.py\", line 42, in wrapped_app\n    |     await app(scope, receive, sender)\n    |   File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/routing.py\", line 714, in __call__\n    |     await self.middleware_stack(scope, receive, send)\n    |   File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/routing.py\", line 734, in app\n    |     await route.handle(scope, receive, send)\n    |   File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/routing.py\", line 288, in handle\n    |     await self.app(scope, receive, send)\n    |   File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/routing.py\", line 76, in app\n    |     await wrap_app_handling_exceptions(app, request)(scope, receive, send)\n    |   File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\n    |     raise exc\n    |   File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/_exception_handler.py\", line 42, in wrapped_app\n    |     await app(scope, receive, sender)\n    |   File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/routing.py\", line 73, in app\n    |     response = await f(request)\n    |   File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/fastapi/routing.py\", line 301, in app\n    |     raw_response = await run_endpoint_function(\n    |   File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/fastapi/routing.py\", line 212, in run_endpoint_function\n    |     return await dependant.call(**values)\n    |   File \"/Users/simba/Downloads/Personal AI Agent/main.py\", line 424, in list_todos\n    |     return todo_service.list_tasks(current_user.username)\n    | TypeError: list_tasks() takes 1 positional argument but 2 were given\n    +------------------------------------\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/middleware/errors.py\", line 165, in __call__\n    await self.app(scope, receive, _send)\n  File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/middleware/cors.py\", line 93, in __call__\n    await self.simple_response(scope, receive, send, request_headers=headers)\n  File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/middleware/cors.py\", line 144, in simple_response\n    await self.app(scope, receive, send)\n  File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/middleware/gzip.py\", line 29, in __call__\n    await responder(scope, receive, send)\n  File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/middleware/gzip.py\", line 126, in __call__\n    await super().__call__(scope, receive, send)\n  File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/middleware/gzip.py\", line 46, in __call__\n    await self.app(scope, receive, self.send_with_compression)\n  File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/middleware/base.py\", line 181, in __call__\n    recv_stream.close()\n  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/contextlib.py\", line 135, in __exit__\n    self.gen.throw(type, value, traceback)\n  File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/_utils.py\", line 82, in collapse_excgroups\n    raise exc\n  File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/middleware/base.py\", line 178, in __call__\n    response = await self.dispatch_func(request, call_next)\n  File \"/Users/simba/Downloads/Personal AI Agent/main.py\", line 145, in __call__\n    response = await call_next(request)\n  File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/middleware/base.py\", line 156, in call_next\n    raise app_exc\n  File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/middleware/base.py\", line 141, in coro\n    await self.app(scope, receive_or_disconnect, send_no_error)\n  File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/middleware/exceptions.py\", line 62, in __call__\n    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)\n  File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\n    raise exc\n  File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/_exception_handler.py\", line 42, in wrapped_app\n    await app(scope, receive, sender)\n  File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/routing.py\", line 714, in __call__\n    await self.middleware_stack(scope, receive, send)\n  File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/routing.py\", line 734, in app\n    await route.handle(scope, receive, send)\n  File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/routing.py\", line 288, in handle\n    await self.app(scope, receive, send)\n  File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/routing.py\", line 76, in app\n    await wrap_app_handling_exceptions(app, request)(scope, receive, send)\n  File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\n    raise exc\n  File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/_exception_handler.py\", line 42, in wrapped_app\n    await app(scope, receive, sender)\n  File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/routing.py\", line 73, in app\n    response = await f(request)\n  File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/fastapi/routing.py\", line 301, in app\n    raw_response = await run_endpoint_function(\n  File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/fastapi/routing.py\", line 212, in run_endpoint_function\n    return await dependant.call(**values)\n  File \"/Users/simba/Downloads/Personal AI Agent/main.py\", line 424, in list_todos\n    return todo_service.list_tasks(current_user.username)\nTypeError: list_tasks() takes 1 positional argument but 2 were given"}
{"timestamp": "2025-04-22T21:02:11.066528", "level": "INFO", "message": "Response sent", "module": "logger", "function": "log_response", "line": 91}
{"timestamp": "2025-04-22T21:02:11.067017", "level": "INFO", "message": "Request: POST /assist - Time: 10.0451s", "module": "main", "function": "__call__", "line": 153}
{"timestamp": "2025-04-22T21:02:13.595915", "level": "ERROR", "message": "Error in api", "module": "logger", "function": "log_error", "line": 108, "exception": "  + Exception Group Traceback (most recent call last):\n  |   File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/_utils.py\", line 76, in collapse_excgroups\n  |     yield\n  |   File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/middleware/base.py\", line 181, in __call__\n  |     recv_stream.close()\n  |   File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/anyio/_backends/_asyncio.py\", line 772, in __aexit__\n  |     raise BaseExceptionGroup(\n  | exceptiongroup.ExceptionGroup: unhandled errors in a TaskGroup (1 sub-exception)\n  +-+---------------- 1 ----------------\n    | Traceback (most recent call last):\n    |   File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/middleware/errors.py\", line 165, in __call__\n    |     await self.app(scope, receive, _send)\n    |   File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/middleware/cors.py\", line 93, in __call__\n    |     await self.simple_response(scope, receive, send, request_headers=headers)\n    |   File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/middleware/cors.py\", line 144, in simple_response\n    |     await self.app(scope, receive, send)\n    |   File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/middleware/gzip.py\", line 29, in __call__\n    |     await responder(scope, receive, send)\n    |   File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/middleware/gzip.py\", line 126, in __call__\n    |     await super().__call__(scope, receive, send)\n    |   File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/middleware/gzip.py\", line 46, in __call__\n    |     await self.app(scope, receive, self.send_with_compression)\n    |   File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/middleware/base.py\", line 181, in __call__\n    |     recv_stream.close()\n    |   File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/contextlib.py\", line 135, in __exit__\n    |     self.gen.throw(type, value, traceback)\n    |   File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/_utils.py\", line 82, in collapse_excgroups\n    |     raise exc\n    |   File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/middleware/base.py\", line 178, in __call__\n    |     response = await self.dispatch_func(request, call_next)\n    |   File \"/Users/simba/Downloads/Personal AI Agent/main.py\", line 145, in __call__\n    |     response = await call_next(request)\n    |   File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/middleware/base.py\", line 156, in call_next\n    |     raise app_exc\n    |   File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/middleware/base.py\", line 141, in coro\n    |     await self.app(scope, receive_or_disconnect, send_no_error)\n    |   File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/middleware/exceptions.py\", line 62, in __call__\n    |     await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)\n    |   File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\n    |     raise exc\n    |   File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/_exception_handler.py\", line 42, in wrapped_app\n    |     await app(scope, receive, sender)\n    |   File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/routing.py\", line 714, in __call__\n    |     await self.middleware_stack(scope, receive, send)\n    |   File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/routing.py\", line 734, in app\n    |     await route.handle(scope, receive, send)\n    |   File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/routing.py\", line 288, in handle\n    |     await self.app(scope, receive, send)\n    |   File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/routing.py\", line 76, in app\n    |     await wrap_app_handling_exceptions(app, request)(scope, receive, send)\n    |   File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\n    |     raise exc\n    |   File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/_exception_handler.py\", line 42, in wrapped_app\n    |     await app(scope, receive, sender)\n    |   File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/routing.py\", line 73, in app\n    |     response = await f(request)\n    |   File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/fastapi/routing.py\", line 301, in app\n    |     raw_response = await run_endpoint_function(\n    |   File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/fastapi/routing.py\", line 212, in run_endpoint_function\n    |     return await dependant.call(**values)\n    |   File \"/Users/simba/Downloads/Personal AI Agent/main.py\", line 424, in list_todos\n    |     return todo_service.list_tasks(current_user.username)\n    | TypeError: list_tasks() takes 1 positional argument but 2 were given\n    +------------------------------------\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/middleware/errors.py\", line 165, in __call__\n    await self.app(scope, receive, _send)\n  File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/middleware/cors.py\", line 93, in __call__\n    await self.simple_response(scope, receive, send, request_headers=headers)\n  File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/middleware/cors.py\", line 144, in simple_response\n    await self.app(scope, receive, send)\n  File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/middleware/gzip.py\", line 29, in __call__\n    await responder(scope, receive, send)\n  File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/middleware/gzip.py\", line 126, in __call__\n    await super().__call__(scope, receive, send)\n  File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/middleware/gzip.py\", line 46, in __call__\n    await self.app(scope, receive, self.send_with_compression)\n  File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/middleware/base.py\", line 181, in __call__\n    recv_stream.close()\n  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/contextlib.py\", line 135, in __exit__\n    self.gen.throw(type, value, traceback)\n  File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/_utils.py\", line 82, in collapse_excgroups\n    raise exc\n  File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/middleware/base.py\", line 178, in __call__\n    response = await self.dispatch_func(request, call_next)\n  File \"/Users/simba/Downloads/Personal AI Agent/main.py\", line 145, in __call__\n    response = await call_next(request)\n  File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/middleware/base.py\", line 156, in call_next\n    raise app_exc\n  File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/middleware/base.py\", line 141, in coro\n    await self.app(scope, receive_or_disconnect, send_no_error)\n  File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/middleware/exceptions.py\", line 62, in __call__\n    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)\n  File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\n    raise exc\n  File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/_exception_handler.py\", line 42, in wrapped_app\n    await app(scope, receive, sender)\n  File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/routing.py\", line 714, in __call__\n    await self.middleware_stack(scope, receive, send)\n  File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/routing.py\", line 734, in app\n    await route.handle(scope, receive, send)\n  File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/routing.py\", line 288, in handle\n    await self.app(scope, receive, send)\n  File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/routing.py\", line 76, in app\n    await wrap_app_handling_exceptions(app, request)(scope, receive, send)\n  File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\n    raise exc\n  File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/_exception_handler.py\", line 42, in wrapped_app\n    await app(scope, receive, sender)\n  File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/routing.py\", line 73, in app\n    response = await f(request)\n  File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/fastapi/routing.py\", line 301, in app\n    raw_response = await run_endpoint_function(\n  File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/fastapi/routing.py\", line 212, in run_endpoint_function\n    return await dependant.call(**values)\n  File \"/Users/simba/Downloads/Personal AI Agent/main.py\", line 424, in list_todos\n    return todo_service.list_tasks(current_user.username)\nTypeError: list_tasks() takes 1 positional argument but 2 were given"}
{"timestamp": "2025-04-22T21:02:13.602996", "level": "ERROR", "message": "Error in api", "module": "logger", "function": "log_error", "line": 108, "exception": "  + Exception Group Traceback (most recent call last):\n  |   File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/_utils.py\", line 76, in collapse_excgroups\n  |     yield\n  |   File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/middleware/base.py\", line 181, in __call__\n  |     recv_stream.close()\n  |   File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/anyio/_backends/_asyncio.py\", line 772, in __aexit__\n  |     raise BaseExceptionGroup(\n  | exceptiongroup.ExceptionGroup: unhandled errors in a TaskGroup (1 sub-exception)\n  +-+---------------- 1 ----------------\n    | Traceback (most recent call last):\n    |   File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/middleware/errors.py\", line 165, in __call__\n    |     await self.app(scope, receive, _send)\n    |   File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/middleware/cors.py\", line 93, in __call__\n    |     await self.simple_response(scope, receive, send, request_headers=headers)\n    |   File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/middleware/cors.py\", line 144, in simple_response\n    |     await self.app(scope, receive, send)\n    |   File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/middleware/gzip.py\", line 29, in __call__\n    |     await responder(scope, receive, send)\n    |   File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/middleware/gzip.py\", line 126, in __call__\n    |     await super().__call__(scope, receive, send)\n    |   File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/middleware/gzip.py\", line 46, in __call__\n    |     await self.app(scope, receive, self.send_with_compression)\n    |   File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/middleware/base.py\", line 181, in __call__\n    |     recv_stream.close()\n    |   File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/contextlib.py\", line 135, in __exit__\n    |     self.gen.throw(type, value, traceback)\n    |   File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/_utils.py\", line 82, in collapse_excgroups\n    |     raise exc\n    |   File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/middleware/base.py\", line 178, in __call__\n    |     response = await self.dispatch_func(request, call_next)\n    |   File \"/Users/simba/Downloads/Personal AI Agent/main.py\", line 145, in __call__\n    |     response = await call_next(request)\n    |   File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/middleware/base.py\", line 156, in call_next\n    |     raise app_exc\n    |   File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/middleware/base.py\", line 141, in coro\n    |     await self.app(scope, receive_or_disconnect, send_no_error)\n    |   File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/middleware/exceptions.py\", line 62, in __call__\n    |     await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)\n    |   File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\n    |     raise exc\n    |   File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/_exception_handler.py\", line 42, in wrapped_app\n    |     await app(scope, receive, sender)\n    |   File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/routing.py\", line 714, in __call__\n    |     await self.middleware_stack(scope, receive, send)\n    |   File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/routing.py\", line 734, in app\n    |     await route.handle(scope, receive, send)\n    |   File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/routing.py\", line 288, in handle\n    |     await self.app(scope, receive, send)\n    |   File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/routing.py\", line 76, in app\n    |     await wrap_app_handling_exceptions(app, request)(scope, receive, send)\n    |   File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\n    |     raise exc\n    |   File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/_exception_handler.py\", line 42, in wrapped_app\n    |     await app(scope, receive, sender)\n    |   File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/routing.py\", line 73, in app\n    |     response = await f(request)\n    |   File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/fastapi/routing.py\", line 301, in app\n    |     raw_response = await run_endpoint_function(\n    |   File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/fastapi/routing.py\", line 212, in run_endpoint_function\n    |     return await dependant.call(**values)\n    |   File \"/Users/simba/Downloads/Personal AI Agent/main.py\", line 424, in list_todos\n    |     return todo_service.list_tasks(current_user.username)\n    | TypeError: list_tasks() takes 1 positional argument but 2 were given\n    +------------------------------------\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/middleware/errors.py\", line 165, in __call__\n    await self.app(scope, receive, _send)\n  File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/middleware/cors.py\", line 93, in __call__\n    await self.simple_response(scope, receive, send, request_headers=headers)\n  File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/middleware/cors.py\", line 144, in simple_response\n    await self.app(scope, receive, send)\n  File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/middleware/gzip.py\", line 29, in __call__\n    await responder(scope, receive, send)\n  File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/middleware/gzip.py\", line 126, in __call__\n    await super().__call__(scope, receive, send)\n  File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/middleware/gzip.py\", line 46, in __call__\n    await self.app(scope, receive, self.send_with_compression)\n  File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/middleware/base.py\", line 181, in __call__\n    recv_stream.close()\n  File \"/Library/Developer/CommandLineTools/Library/Frameworks/Python3.framework/Versions/3.9/lib/python3.9/contextlib.py\", line 135, in __exit__\n    self.gen.throw(type, value, traceback)\n  File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/_utils.py\", line 82, in collapse_excgroups\n    raise exc\n  File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/middleware/base.py\", line 178, in __call__\n    response = await self.dispatch_func(request, call_next)\n  File \"/Users/simba/Downloads/Personal AI Agent/main.py\", line 145, in __call__\n    response = await call_next(request)\n  File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/middleware/base.py\", line 156, in call_next\n    raise app_exc\n  File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/middleware/base.py\", line 141, in coro\n    await self.app(scope, receive_or_disconnect, send_no_error)\n  File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/middleware/exceptions.py\", line 62, in __call__\n    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)\n  File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\n    raise exc\n  File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/_exception_handler.py\", line 42, in wrapped_app\n    await app(scope, receive, sender)\n  File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/routing.py\", line 714, in __call__\n    await self.middleware_stack(scope, receive, send)\n  File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/routing.py\", line 734, in app\n    await route.handle(scope, receive, send)\n  File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/routing.py\", line 288, in handle\n    await self.app(scope, receive, send)\n  File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/routing.py\", line 76, in app\n    await wrap_app_handling_exceptions(app, request)(scope, receive, send)\n  File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/_exception_handler.py\", line 53, in wrapped_app\n    raise exc\n  File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/_exception_handler.py\", line 42, in wrapped_app\n    await app(scope, receive, sender)\n  File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/starlette/routing.py\", line 73, in app\n    response = await f(request)\n  File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/fastapi/routing.py\", line 301, in app\n    raw_response = await run_endpoint_function(\n  File \"/Users/simba/Downloads/Personal AI Agent/.venv/lib/python3.9/site-packages/fastapi/routing.py\", line 212, in run_endpoint_function\n    return await dependant.call(**values)\n  File \"/Users/simba/Downloads/Personal AI Agent/main.py\", line 424, in list_todos\n    return todo_service.list_tasks(current_user.username)\nTypeError: list_tasks() takes 1 positional argument but 2 were given"}
{"timestamp": "2025-04-22T21:04:17.385228", "level": "INFO", "message": "Request received", "module": "logger", "function": "log_request", "line": 74}
{"timestamp": "2025-04-22T21:04:29.763388", "level": "INFO", "message": "Response sent", "module": "logger", "function": "log_response", "line": 91}
{"timestamp": "2025-04-22T21:04:29.764601", "level": "INFO", "message": "Request: POST /assist - Time: 12.3804s", "module": "main", "function": "__call__", "line": 153}
{"timestamp": "2025-04-22T21:04:29.828122", "level": "INFO", "message": "Closing LLM client...", "module": "main", "function": "shutdown_event", "line": 228}
{"timestamp": "2025-04-22T21:04:29.829027", "level": "INFO", "message": "LLM client closed. Application shutting down.", "module": "main", "function": "shutdown_event", "line": 230}
{"timestamp": "2025-04-22T21:04:30.662511", "level": "INFO", "message": "\ud83d\ude80 Starting LLM Assistant API", "module": "main", "function": "startup_event", "line": 222}
{"timestamp": "2025-04-22T21:04:30.667589", "level": "INFO", "message": "Request received", "module": "logger", "function": "log_request", "line": 74}
{"timestamp": "2025-04-22T21:04:42.081228", "level": "INFO", "message": "Response sent", "module": "logger", "function": "log_response", "line": 91}
{"timestamp": "2025-04-22T21:04:42.081499", "level": "INFO", "message": "Request: POST /assist - Time: 11.4165s", "module": "main", "function": "__call__", "line": 153}
{"timestamp": "2025-04-22T21:05:04.421709", "level": "INFO", "message": "Closing LLM client...", "module": "main", "function": "shutdown_event", "line": 228}
{"timestamp": "2025-04-22T21:05:04.422210", "level": "INFO", "message": "LLM client closed. Application shutting down.", "module": "main", "function": "shutdown_event", "line": 230}
{"timestamp": "2025-04-22T21:05:05.076232", "level": "INFO", "message": "\ud83d\ude80 Starting LLM Assistant API", "module": "main", "function": "startup_event", "line": 222}
{"timestamp": "2025-04-22T21:05:24.281728", "level": "INFO", "message": "Closing LLM client...", "module": "main", "function": "shutdown_event", "line": 228}
{"timestamp": "2025-04-22T21:05:24.283848", "level": "INFO", "message": "LLM client closed. Application shutting down.", "module": "main", "function": "shutdown_event", "line": 230}
{"timestamp": "2025-04-22T21:05:24.949678", "level": "INFO", "message": "\ud83d\ude80 Starting LLM Assistant API", "module": "main", "function": "startup_event", "line": 222}
{"timestamp": "2025-04-22T21:06:13.658638", "level": "INFO", "message": "Request received", "module": "logger", "function": "log_request", "line": 74}
{"timestamp": "2025-04-22T21:06:24.139096", "level": "INFO", "message": "Request: GET /todos - Time: 0.0024s", "module": "main", "function": "__call__", "line": 153}
{"timestamp": "2025-04-22T21:06:24.143983", "level": "INFO", "message": "Request: GET /todos - Time: 0.0010s", "module": "main", "function": "__call__", "line": 153}
{"timestamp": "2025-04-22T21:06:24.195230", "level": "INFO", "message": "Response sent", "module": "logger", "function": "log_response", "line": 91}
{"timestamp": "2025-04-22T21:06:24.195545", "level": "INFO", "message": "Request: POST /assist - Time: 10.5421s", "module": "main", "function": "__call__", "line": 153}
{"timestamp": "2025-04-22T21:06:35.765007", "level": "INFO", "message": "Request received", "module": "logger", "function": "log_request", "line": 74}
{"timestamp": "2025-04-22T21:06:48.476444", "level": "INFO", "message": "Response sent", "module": "logger", "function": "log_response", "line": 91}
{"timestamp": "2025-04-22T21:06:48.476947", "level": "INFO", "message": "Request: POST /assist - Time: 12.7144s", "module": "main", "function": "__call__", "line": 153}
{"timestamp": "2025-04-22T21:07:17.237651", "level": "INFO", "message": "Closing LLM client...", "module": "main", "function": "shutdown_event", "line": 228}
{"timestamp": "2025-04-22T21:07:17.237864", "level": "INFO", "message": "LLM client closed. Application shutting down.", "module": "main", "function": "shutdown_event", "line": 230}
{"timestamp": "2025-04-22T21:07:23.644792", "level": "INFO", "message": "\ud83d\ude80 Starting LLM Assistant API", "module": "main", "function": "startup_event", "line": 222}
{"timestamp": "2025-04-22T21:07:47.370972", "level": "INFO", "message": "Request received", "module": "logger", "function": "log_request", "line": 74}
{"timestamp": "2025-04-22T21:07:58.934481", "level": "INFO", "message": "Response sent", "module": "logger", "function": "log_response", "line": 91}
{"timestamp": "2025-04-22T21:07:58.935262", "level": "INFO", "message": "Request: POST /assist - Time: 11.5719s", "module": "main", "function": "__call__", "line": 153}
{"timestamp": "2025-04-22T21:10:07.174997", "level": "INFO", "message": "Closing LLM client...", "module": "main", "function": "shutdown_event", "line": 228}
{"timestamp": "2025-04-22T21:10:07.175231", "level": "INFO", "message": "LLM client closed. Application shutting down.", "module": "main", "function": "shutdown_event", "line": 230}
{"timestamp": "2025-04-22T21:10:07.911187", "level": "INFO", "message": "\ud83d\ude80 Starting LLM Assistant API", "module": "main", "function": "startup_event", "line": 222}
{"timestamp": "2025-04-22T21:10:28.457970", "level": "INFO", "message": "Request: GET /llm-test - Time: 15.9131s", "module": "main", "function": "__call__", "line": 153}
{"timestamp": "2025-04-22T21:10:40.606681", "level": "INFO", "message": "Request received", "module": "logger", "function": "log_request", "line": 74}
{"timestamp": "2025-04-22T21:10:56.080066", "level": "INFO", "message": "Response sent", "module": "logger", "function": "log_response", "line": 91}
{"timestamp": "2025-04-22T21:10:56.080813", "level": "INFO", "message": "Request: POST /assist - Time: 15.4829s", "module": "main", "function": "__call__", "line": 153}
{"timestamp": "2025-04-22T21:11:33.056866", "level": "INFO", "message": "Request: GET /llm-test - Time: 11.2713s", "module": "main", "function": "__call__", "line": 153}
{"timestamp": "2025-04-22T21:12:07.942295", "level": "INFO", "message": "Closing LLM client...", "module": "main", "function": "shutdown_event", "line": 228}
{"timestamp": "2025-04-22T21:12:07.942650", "level": "INFO", "message": "LLM client closed. Application shutting down.", "module": "main", "function": "shutdown_event", "line": 230}
{"timestamp": "2025-04-22T21:12:08.594787", "level": "INFO", "message": "\ud83d\ude80 Starting LLM Assistant API", "module": "main", "function": "startup_event", "line": 222}
{"timestamp": "2025-04-22T21:12:15.061673", "level": "INFO", "message": "Request: GET /llm-test - Time: 1.7957s", "module": "main", "function": "__call__", "line": 153}
{"timestamp": "2025-04-22T21:13:27.070482", "level": "INFO", "message": "Closing LLM client...", "module": "main", "function": "shutdown_event", "line": 228}
{"timestamp": "2025-04-22T21:13:27.070664", "level": "INFO", "message": "LLM client closed. Application shutting down.", "module": "main", "function": "shutdown_event", "line": 230}
{"timestamp": "2025-04-22T21:13:29.717459", "level": "INFO", "message": "\ud83d\ude80 Starting LLM Assistant API", "module": "main", "function": "startup_event", "line": 222}
{"timestamp": "2025-04-22T21:13:35.985332", "level": "INFO", "message": "Request: GET /llm-test - Time: 1.6406s", "module": "main", "function": "__call__", "line": 153}
{"timestamp": "2025-04-22T21:13:44.359372", "level": "INFO", "message": "Request received", "module": "logger", "function": "log_request", "line": 74}
{"timestamp": "2025-04-22T21:13:46.064618", "level": "INFO", "message": "Response sent", "module": "logger", "function": "log_response", "line": 91}
{"timestamp": "2025-04-22T21:13:46.064855", "level": "INFO", "message": "Request: POST /assist - Time: 1.7088s", "module": "main", "function": "__call__", "line": 153}
{"timestamp": "2025-04-22T21:13:48.334041", "level": "INFO", "message": "Request received", "module": "logger", "function": "log_request", "line": 74}
{"timestamp": "2025-04-22T21:13:49.802975", "level": "INFO", "message": "Response sent", "module": "logger", "function": "log_response", "line": 91}
{"timestamp": "2025-04-22T21:13:49.803195", "level": "INFO", "message": "Request: POST /assist - Time: 1.4701s", "module": "main", "function": "__call__", "line": 153}
{"timestamp": "2025-04-22T21:15:43.356665", "level": "INFO", "message": "Closing LLM client...", "module": "main", "function": "shutdown_event", "line": 228}
{"timestamp": "2025-04-22T21:15:43.357968", "level": "INFO", "message": "LLM client closed. Application shutting down.", "module": "main", "function": "shutdown_event", "line": 230}
{"timestamp": "2025-04-22T21:15:44.154097", "level": "INFO", "message": "\ud83d\ude80 Starting LLM Assistant API", "module": "main", "function": "startup_event", "line": 222}
{"timestamp": "2025-04-22T21:19:47.849465", "level": "INFO", "message": "Closing LLM client...", "module": "main", "function": "shutdown_event", "line": 228}
{"timestamp": "2025-04-22T21:19:47.888184", "level": "INFO", "message": "LLM client closed. Application shutting down.", "module": "main", "function": "shutdown_event", "line": 230}
{"timestamp": "2025-04-22T21:21:13.975070", "level": "INFO", "message": "\ud83d\ude80 Starting LLM Assistant API", "module": "main", "function": "startup_event", "line": 222}
{"timestamp": "2025-04-22T21:21:25.197288", "level": "INFO", "message": "Closing LLM client...", "module": "main", "function": "shutdown_event", "line": 228}
{"timestamp": "2025-04-22T21:21:25.200085", "level": "INFO", "message": "LLM client closed. Application shutting down.", "module": "main", "function": "shutdown_event", "line": 230}
{"timestamp": "2025-04-22T21:43:05.888436", "level": "INFO", "message": "\ud83d\ude80 Starting LLM Assistant API", "module": "main", "function": "startup_event", "line": 222}
{"timestamp": "2025-04-22T21:43:20.253521", "level": "INFO", "message": "Closing LLM client...", "module": "main", "function": "shutdown_event", "line": 228}
{"timestamp": "2025-04-22T21:43:20.255860", "level": "INFO", "message": "LLM client closed. Application shutting down.", "module": "main", "function": "shutdown_event", "line": 230}
{"timestamp": "2025-04-22T21:43:24.495075", "level": "INFO", "message": "\ud83d\ude80 Starting LLM Assistant API", "module": "main", "function": "startup_event", "line": 222}
{"timestamp": "2025-04-22T21:44:40.440903", "level": "INFO", "message": "Successful login for user: testuser", "module": "main", "function": "login_for_access_token", "line": 288}
{"timestamp": "2025-04-22T21:44:40.444913", "level": "INFO", "message": "Request: POST /token - Time: 0.2560s", "module": "main", "function": "__call__", "line": 153}
{"timestamp": "2025-04-22T21:44:50.148633", "level": "INFO", "message": "Request received", "module": "logger", "function": "log_request", "line": 74}
{"timestamp": "2025-04-22T21:44:50.539513", "level": "INFO", "message": "Response sent", "module": "logger", "function": "log_response", "line": 91}
{"timestamp": "2025-04-22T21:44:50.540086", "level": "INFO", "message": "Request: POST /assist - Time: 0.3933s", "module": "main", "function": "__call__", "line": 153}
{"timestamp": "2025-04-22T21:45:48.007513", "level": "INFO", "message": "Closing LLM client...", "module": "main", "function": "shutdown_event", "line": 228}
{"timestamp": "2025-04-22T21:45:48.007739", "level": "INFO", "message": "LLM client closed. Application shutting down.", "module": "main", "function": "shutdown_event", "line": 230}
{"timestamp": "2025-04-22T21:45:48.648095", "level": "INFO", "message": "\ud83d\ude80 Starting LLM Assistant API", "module": "main", "function": "startup_event", "line": 222}
{"timestamp": "2025-04-22T21:46:12.090672", "level": "INFO", "message": "Request received", "module": "logger", "function": "log_request", "line": 74}
{"timestamp": "2025-04-22T21:46:12.352958", "level": "INFO", "message": "Response sent", "module": "logger", "function": "log_response", "line": 91}
{"timestamp": "2025-04-22T21:46:12.353387", "level": "INFO", "message": "Request: POST /assist - Time: 0.2700s", "module": "main", "function": "__call__", "line": 153}
{"timestamp": "2025-04-22T21:46:13.564261", "level": "INFO", "message": "Request received", "module": "logger", "function": "log_request", "line": 74}
{"timestamp": "2025-04-22T21:46:13.689881", "level": "INFO", "message": "Response sent", "module": "logger", "function": "log_response", "line": 91}
{"timestamp": "2025-04-22T21:46:13.690266", "level": "INFO", "message": "Request: POST /assist - Time: 0.1282s", "module": "main", "function": "__call__", "line": 153}
{"timestamp": "2025-04-22T21:46:14.371880", "level": "INFO", "message": "Request received", "module": "logger", "function": "log_request", "line": 74}
{"timestamp": "2025-04-22T21:46:14.492206", "level": "INFO", "message": "Response sent", "module": "logger", "function": "log_response", "line": 91}
{"timestamp": "2025-04-22T21:46:14.492588", "level": "INFO", "message": "Request: POST /assist - Time: 0.1226s", "module": "main", "function": "__call__", "line": 153}
{"timestamp": "2025-04-22T21:46:47.713000", "level": "INFO", "message": "Closing LLM client...", "module": "main", "function": "shutdown_event", "line": 228}
{"timestamp": "2025-04-22T21:46:47.713260", "level": "INFO", "message": "LLM client closed. Application shutting down.", "module": "main", "function": "shutdown_event", "line": 230}
{"timestamp": "2025-04-22T21:46:48.370090", "level": "INFO", "message": "\ud83d\ude80 Starting LLM Assistant API", "module": "main", "function": "startup_event", "line": 222}
{"timestamp": "2025-04-22T21:47:14.275759", "level": "INFO", "message": "Request received", "module": "logger", "function": "log_request", "line": 74}
{"timestamp": "2025-04-22T21:47:15.110740", "level": "INFO", "message": "Response sent", "module": "logger", "function": "log_response", "line": 91}
{"timestamp": "2025-04-22T21:47:15.111329", "level": "INFO", "message": "Request: POST /assist - Time: 0.8422s", "module": "main", "function": "__call__", "line": 153}
{"timestamp": "2025-04-22T21:47:26.608844", "level": "INFO", "message": "Request received", "module": "logger", "function": "log_request", "line": 74}
{"timestamp": "2025-04-22T21:47:27.297569", "level": "INFO", "message": "Response sent", "module": "logger", "function": "log_response", "line": 91}
{"timestamp": "2025-04-22T21:47:27.298313", "level": "INFO", "message": "Request: POST /assist - Time: 0.6922s", "module": "main", "function": "__call__", "line": 153}
{"timestamp": "2025-04-22T21:47:37.394885", "level": "INFO", "message": "Request received", "module": "logger", "function": "log_request", "line": 74}
{"timestamp": "2025-04-22T21:47:39.279652", "level": "INFO", "message": "Response sent", "module": "logger", "function": "log_response", "line": 91}
{"timestamp": "2025-04-22T21:47:39.280666", "level": "INFO", "message": "Request: POST /assist - Time: 1.8873s", "module": "main", "function": "__call__", "line": 153}
{"timestamp": "2025-04-22T21:47:45.771557", "level": "INFO", "message": "Request: GET /todos - Time: 0.0013s", "module": "main", "function": "__call__", "line": 153}
{"timestamp": "2025-04-22T21:47:45.774750", "level": "INFO", "message": "Request: GET /todos - Time: 0.0010s", "module": "main", "function": "__call__", "line": 153}
{"timestamp": "2025-04-22T21:47:54.376165", "level": "INFO", "message": "Request received", "module": "logger", "function": "log_request", "line": 74}
{"timestamp": "2025-04-22T21:47:54.381908", "level": "ERROR", "message": "Error in todo", "module": "logger", "function": "log_error", "line": 108, "exception": "Traceback (most recent call last):\n  File \"/Users/simba/Downloads/Personal AI Agent/main.py\", line 366, in process_request\n    response = await todo_service.process(message.content, message.parameters)\n  File \"/Users/simba/Downloads/Personal AI Agent/services/todo_manager.py\", line 78, in process\n    command = content.lower().strip()\nAttributeError: 'dict' object has no attribute 'lower'"}
{"timestamp": "2025-04-22T21:47:54.382708", "level": "INFO", "message": "Request: POST /assist - Time: 0.0095s", "module": "main", "function": "__call__", "line": 153}
{"timestamp": "2025-04-22T21:47:56.109382", "level": "INFO", "message": "Request received", "module": "logger", "function": "log_request", "line": 74}
{"timestamp": "2025-04-22T21:47:56.111111", "level": "ERROR", "message": "Error in todo", "module": "logger", "function": "log_error", "line": 108, "exception": "Traceback (most recent call last):\n  File \"/Users/simba/Downloads/Personal AI Agent/main.py\", line 366, in process_request\n    response = await todo_service.process(message.content, message.parameters)\n  File \"/Users/simba/Downloads/Personal AI Agent/services/todo_manager.py\", line 78, in process\n    command = content.lower().strip()\nAttributeError: 'dict' object has no attribute 'lower'"}
{"timestamp": "2025-04-22T21:47:56.111799", "level": "INFO", "message": "Request: POST /assist - Time: 0.0050s", "module": "main", "function": "__call__", "line": 153}
{"timestamp": "2025-04-22T21:47:57.428460", "level": "INFO", "message": "Request received", "module": "logger", "function": "log_request", "line": 74}
{"timestamp": "2025-04-22T21:47:57.429699", "level": "ERROR", "message": "Error in todo", "module": "logger", "function": "log_error", "line": 108, "exception": "Traceback (most recent call last):\n  File \"/Users/simba/Downloads/Personal AI Agent/main.py\", line 366, in process_request\n    response = await todo_service.process(message.content, message.parameters)\n  File \"/Users/simba/Downloads/Personal AI Agent/services/todo_manager.py\", line 78, in process\n    command = content.lower().strip()\nAttributeError: 'dict' object has no attribute 'lower'"}
{"timestamp": "2025-04-22T21:47:57.430571", "level": "INFO", "message": "Request: POST /assist - Time: 0.0054s", "module": "main", "function": "__call__", "line": 153}
{"timestamp": "2025-04-22T21:48:03.611594", "level": "INFO", "message": "Request received", "module": "logger", "function": "log_request", "line": 74}
{"timestamp": "2025-04-22T21:48:09.933793", "level": "INFO", "message": "Response sent", "module": "logger", "function": "log_response", "line": 91}
{"timestamp": "2025-04-22T21:48:09.934716", "level": "INFO", "message": "Request: POST /assist - Time: 6.3246s", "module": "main", "function": "__call__", "line": 153}
{"timestamp": "2025-04-22T21:48:23.033864", "level": "INFO", "message": "Request received", "module": "logger", "function": "log_request", "line": 74}
{"timestamp": "2025-04-22T21:48:23.567280", "level": "INFO", "message": "Response sent", "module": "logger", "function": "log_response", "line": 91}
{"timestamp": "2025-04-22T21:48:23.568037", "level": "INFO", "message": "Request: POST /assist - Time: 0.5369s", "module": "main", "function": "__call__", "line": 153}
{"timestamp": "2025-04-22T21:48:35.407254", "level": "INFO", "message": "Request received", "module": "logger", "function": "log_request", "line": 74}
{"timestamp": "2025-04-22T21:48:36.067190", "level": "INFO", "message": "Response sent", "module": "logger", "function": "log_response", "line": 91}
{"timestamp": "2025-04-22T21:48:36.067833", "level": "INFO", "message": "Request: POST /assist - Time: 0.6622s", "module": "main", "function": "__call__", "line": 153}
{"timestamp": "2025-04-22T21:48:52.866300", "level": "INFO", "message": "Request received", "module": "logger", "function": "log_request", "line": 74}
{"timestamp": "2025-04-22T21:48:53.880077", "level": "INFO", "message": "Response sent", "module": "logger", "function": "log_response", "line": 91}
{"timestamp": "2025-04-22T21:48:53.880676", "level": "INFO", "message": "Request: POST /assist - Time: 1.0171s", "module": "main", "function": "__call__", "line": 153}
{"timestamp": "2025-04-22T21:49:08.643279", "level": "INFO", "message": "Request received", "module": "logger", "function": "log_request", "line": 74}
{"timestamp": "2025-04-22T21:49:09.338052", "level": "INFO", "message": "Response sent", "module": "logger", "function": "log_response", "line": 91}
{"timestamp": "2025-04-22T21:49:09.338573", "level": "INFO", "message": "Request: POST /assist - Time: 0.6981s", "module": "main", "function": "__call__", "line": 153}
{"timestamp": "2025-04-22T21:49:15.774655", "level": "INFO", "message": "Request received", "module": "logger", "function": "log_request", "line": 74}
{"timestamp": "2025-04-22T21:49:17.228036", "level": "INFO", "message": "Response sent", "module": "logger", "function": "log_response", "line": 91}
{"timestamp": "2025-04-22T21:49:17.228714", "level": "INFO", "message": "Request: POST /assist - Time: 1.4554s", "module": "main", "function": "__call__", "line": 153}
{"timestamp": "2025-04-22T21:49:31.037387", "level": "INFO", "message": "Request received", "module": "logger", "function": "log_request", "line": 74}
{"timestamp": "2025-04-22T21:49:31.763249", "level": "INFO", "message": "Response sent", "module": "logger", "function": "log_response", "line": 91}
{"timestamp": "2025-04-22T21:49:31.763714", "level": "INFO", "message": "Request: POST /assist - Time: 0.7299s", "module": "main", "function": "__call__", "line": 153}
{"timestamp": "2025-04-22T21:50:00.280092", "level": "INFO", "message": "Closing LLM client...", "module": "main", "function": "shutdown_event", "line": 228}
{"timestamp": "2025-04-22T21:50:00.280528", "level": "INFO", "message": "LLM client closed. Application shutting down.", "module": "main", "function": "shutdown_event", "line": 230}
{"timestamp": "2025-04-22T21:52:48.152220", "level": "INFO", "message": "\ud83d\ude80 Starting LLM Assistant API", "module": "main", "function": "startup_event", "line": 222}
{"timestamp": "2025-04-22T21:52:59.984030", "level": "INFO", "message": "Closing LLM client...", "module": "main", "function": "shutdown_event", "line": 228}
{"timestamp": "2025-04-22T21:52:59.986087", "level": "INFO", "message": "LLM client closed. Application shutting down.", "module": "main", "function": "shutdown_event", "line": 230}
{"timestamp": "2025-04-22T21:53:02.899942", "level": "INFO", "message": "\ud83d\ude80 Starting LLM Assistant API", "module": "main", "function": "startup_event", "line": 222}
{"timestamp": "2025-04-22T21:54:18.655985", "level": "INFO", "message": "Request received", "module": "logger", "function": "log_request", "line": 74}
{"timestamp": "2025-04-22T21:54:19.426499", "level": "INFO", "message": "Response sent", "module": "logger", "function": "log_response", "line": 91}
{"timestamp": "2025-04-22T21:54:19.427033", "level": "INFO", "message": "Request: POST /assist - Time: 0.7747s", "module": "main", "function": "__call__", "line": 153}
{"timestamp": "2025-04-22T21:54:28.066607", "level": "INFO", "message": "Request received", "module": "logger", "function": "log_request", "line": 74}
{"timestamp": "2025-04-22T21:54:28.066861", "level": "INFO", "message": "Response sent", "module": "logger", "function": "log_response", "line": 91}
{"timestamp": "2025-04-22T21:54:28.068005", "level": "INFO", "message": "Request: POST /assist - Time: 0.0040s", "module": "main", "function": "__call__", "line": 153}
{"timestamp": "2025-04-22T21:54:33.690086", "level": "INFO", "message": "Request: GET /todos - Time: 0.0025s", "module": "main", "function": "__call__", "line": 153}
{"timestamp": "2025-04-22T21:54:33.694771", "level": "INFO", "message": "Request: GET /todos - Time: 0.0010s", "module": "main", "function": "__call__", "line": 153}
{"timestamp": "2025-04-22T21:54:40.036399", "level": "INFO", "message": "Request received", "module": "logger", "function": "log_request", "line": 74}
{"timestamp": "2025-04-22T21:54:43.583961", "level": "INFO", "message": "Response sent", "module": "logger", "function": "log_response", "line": 91}
{"timestamp": "2025-04-22T21:54:43.584691", "level": "INFO", "message": "Request: POST /assist - Time: 3.5503s", "module": "main", "function": "__call__", "line": 153}
{"timestamp": "2025-04-22T21:54:52.451699", "level": "INFO", "message": "Request received", "module": "logger", "function": "log_request", "line": 74}
{"timestamp": "2025-04-22T21:54:53.001562", "level": "INFO", "message": "Response sent", "module": "logger", "function": "log_response", "line": 91}
{"timestamp": "2025-04-22T21:54:53.002163", "level": "INFO", "message": "Request: POST /assist - Time: 0.5527s", "module": "main", "function": "__call__", "line": 153}
{"timestamp": "2025-04-22T21:55:00.901339", "level": "INFO", "message": "Request received", "module": "logger", "function": "log_request", "line": 74}
{"timestamp": "2025-04-22T21:55:01.807715", "level": "INFO", "message": "Response sent", "module": "logger", "function": "log_response", "line": 91}
{"timestamp": "2025-04-22T21:55:01.808404", "level": "INFO", "message": "Request: POST /assist - Time: 0.9099s", "module": "main", "function": "__call__", "line": 153}
{"timestamp": "2025-04-22T21:55:15.278677", "level": "INFO", "message": "Request received", "module": "logger", "function": "log_request", "line": 74}
{"timestamp": "2025-04-22T21:55:15.938692", "level": "INFO", "message": "Response sent", "module": "logger", "function": "log_response", "line": 91}
{"timestamp": "2025-04-22T21:55:15.939273", "level": "INFO", "message": "Request: POST /assist - Time: 0.6634s", "module": "main", "function": "__call__", "line": 153}
{"timestamp": "2025-04-22T21:55:36.787280", "level": "INFO", "message": "Request received", "module": "logger", "function": "log_request", "line": 74}
{"timestamp": "2025-04-22T21:55:37.439377", "level": "INFO", "message": "Response sent", "module": "logger", "function": "log_response", "line": 91}
{"timestamp": "2025-04-22T21:55:37.439947", "level": "INFO", "message": "Request: POST /assist - Time: 0.6547s", "module": "main", "function": "__call__", "line": 153}
{"timestamp": "2025-04-22T21:55:43.618872", "level": "INFO", "message": "Request received", "module": "logger", "function": "log_request", "line": 74}
{"timestamp": "2025-04-22T21:55:44.198873", "level": "INFO", "message": "Response sent", "module": "logger", "function": "log_response", "line": 91}
{"timestamp": "2025-04-22T21:55:44.199430", "level": "INFO", "message": "Request: POST /assist - Time: 0.5826s", "module": "main", "function": "__call__", "line": 153}
{"timestamp": "2025-04-22T21:55:48.506594", "level": "INFO", "message": "Request received", "module": "logger", "function": "log_request", "line": 74}
{"timestamp": "2025-04-22T21:55:48.909615", "level": "INFO", "message": "Response sent", "module": "logger", "function": "log_response", "line": 91}
{"timestamp": "2025-04-22T21:55:48.910268", "level": "INFO", "message": "Request: POST /assist - Time: 0.4060s", "module": "main", "function": "__call__", "line": 153}
{"timestamp": "2025-04-22T21:55:52.998176", "level": "INFO", "message": "Request received", "module": "logger", "function": "log_request", "line": 74}
{"timestamp": "2025-04-22T21:55:53.311569", "level": "INFO", "message": "Response sent", "module": "logger", "function": "log_response", "line": 91}
{"timestamp": "2025-04-22T21:55:53.312124", "level": "INFO", "message": "Request: POST /assist - Time: 0.3156s", "module": "main", "function": "__call__", "line": 153}
{"timestamp": "2025-04-22T21:55:56.680009", "level": "INFO", "message": "Request received", "module": "logger", "function": "log_request", "line": 74}
{"timestamp": "2025-04-22T21:55:57.101907", "level": "INFO", "message": "Response sent", "module": "logger", "function": "log_response", "line": 91}
{"timestamp": "2025-04-22T21:55:57.102392", "level": "INFO", "message": "Request: POST /assist - Time: 0.4251s", "module": "main", "function": "__call__", "line": 153}
{"timestamp": "2025-04-22T21:56:11.735567", "level": "INFO", "message": "Request: GET /todos - Time: 0.0022s", "module": "main", "function": "__call__", "line": 153}
{"timestamp": "2025-04-22T21:56:11.740738", "level": "INFO", "message": "Request: GET /todos - Time: 0.0015s", "module": "main", "function": "__call__", "line": 153}
{"timestamp": "2025-04-22T21:59:37.456958", "level": "INFO", "message": "Request received", "module": "logger", "function": "log_request", "line": 74}
{"timestamp": "2025-04-22T21:59:37.457054", "level": "INFO", "message": "Response sent", "module": "logger", "function": "log_response", "line": 91}
{"timestamp": "2025-04-22T21:59:37.457360", "level": "INFO", "message": "Request: POST /assist - Time: 0.0011s", "module": "main", "function": "__call__", "line": 153}
{"timestamp": "2025-04-22T21:59:50.141585", "level": "INFO", "message": "Request received", "module": "logger", "function": "log_request", "line": 74}
{"timestamp": "2025-04-22T21:59:51.422218", "level": "INFO", "message": "Response sent", "module": "logger", "function": "log_response", "line": 91}
{"timestamp": "2025-04-22T21:59:51.422807", "level": "INFO", "message": "Request: POST /assist - Time: 1.2829s", "module": "main", "function": "__call__", "line": 153}
{"timestamp": "2025-04-22T22:03:39.697179", "level": "INFO", "message": "Request received", "module": "logger", "function": "log_request", "line": 74}
{"timestamp": "2025-04-22T22:03:39.697571", "level": "INFO", "message": "Response sent", "module": "logger", "function": "log_response", "line": 91}
{"timestamp": "2025-04-22T22:03:39.698331", "level": "INFO", "message": "Request: POST /assist - Time: 0.0035s", "module": "main", "function": "__call__", "line": 153}
{"timestamp": "2025-04-22T22:03:42.816129", "level": "INFO", "message": "Request: GET /todos - Time: 0.0017s", "module": "main", "function": "__call__", "line": 153}
{"timestamp": "2025-04-22T22:03:42.820301", "level": "INFO", "message": "Request: GET /todos - Time: 0.0010s", "module": "main", "function": "__call__", "line": 153}
{"timestamp": "2025-04-22T22:03:52.209932", "level": "INFO", "message": "Request received", "module": "logger", "function": "log_request", "line": 74}
{"timestamp": "2025-04-22T22:03:52.759019", "level": "INFO", "message": "Response sent", "module": "logger", "function": "log_response", "line": 91}
{"timestamp": "2025-04-22T22:03:52.759673", "level": "INFO", "message": "Request: POST /assist - Time: 0.5513s", "module": "main", "function": "__call__", "line": 153}
{"timestamp": "2025-04-22T22:03:56.085218", "level": "INFO", "message": "Request received", "module": "logger", "function": "log_request", "line": 74}
{"timestamp": "2025-04-22T22:03:56.443221", "level": "INFO", "message": "Response sent", "module": "logger", "function": "log_response", "line": 91}
{"timestamp": "2025-04-22T22:03:56.443742", "level": "INFO", "message": "Request: POST /assist - Time: 0.3609s", "module": "main", "function": "__call__", "line": 153}
{"timestamp": "2025-04-22T22:03:58.919279", "level": "INFO", "message": "Request received", "module": "logger", "function": "log_request", "line": 74}
{"timestamp": "2025-04-22T22:03:59.306887", "level": "INFO", "message": "Response sent", "module": "logger", "function": "log_response", "line": 91}
{"timestamp": "2025-04-22T22:03:59.307365", "level": "INFO", "message": "Request: POST /assist - Time: 0.3900s", "module": "main", "function": "__call__", "line": 153}
{"timestamp": "2025-04-22T22:04:02.550289", "level": "INFO", "message": "Request received", "module": "logger", "function": "log_request", "line": 74}
{"timestamp": "2025-04-22T22:04:03.403249", "level": "INFO", "message": "Response sent", "module": "logger", "function": "log_response", "line": 91}
{"timestamp": "2025-04-22T22:04:03.403646", "level": "INFO", "message": "Request: POST /assist - Time: 0.8562s", "module": "main", "function": "__call__", "line": 153}
{"timestamp": "2025-04-22T22:07:08.450098", "level": "INFO", "message": "Request received", "module": "logger", "function": "log_request", "line": 74}
{"timestamp": "2025-04-22T22:07:08.450819", "level": "INFO", "message": "Response sent", "module": "logger", "function": "log_response", "line": 91}
{"timestamp": "2025-04-22T22:07:08.451402", "level": "INFO", "message": "Request: POST /assist - Time: 0.0029s", "module": "main", "function": "__call__", "line": 153}
{"timestamp": "2025-04-22T22:07:18.554009", "level": "INFO", "message": "Request received", "module": "logger", "function": "log_request", "line": 74}
{"timestamp": "2025-04-22T22:07:19.102772", "level": "INFO", "message": "Response sent", "module": "logger", "function": "log_response", "line": 91}
{"timestamp": "2025-04-22T22:07:19.103512", "level": "INFO", "message": "Request: POST /assist - Time: 0.5514s", "module": "main", "function": "__call__", "line": 153}
{"timestamp": "2025-04-22T22:15:23.283079", "level": "INFO", "message": "Closing LLM client...", "module": "main", "function": "shutdown_event", "line": 228}
{"timestamp": "2025-04-22T22:15:23.283432", "level": "INFO", "message": "LLM client closed. Application shutting down.", "module": "main", "function": "shutdown_event", "line": 230}
{"timestamp": "2025-04-22T22:15:24.104301", "level": "INFO", "message": "\ud83d\ude80 Starting LLM Assistant API", "module": "main", "function": "startup_event", "line": 222}
{"timestamp": "2025-04-22T22:17:08.403933", "level": "INFO", "message": "Closing LLM client...", "module": "main", "function": "shutdown_event", "line": 228}
{"timestamp": "2025-04-22T22:17:08.407586", "level": "INFO", "message": "LLM client closed. Application shutting down.", "module": "main", "function": "shutdown_event", "line": 230}
{"timestamp": "2025-04-22T22:17:09.060380", "level": "INFO", "message": "\ud83d\ude80 Starting LLM Assistant API", "module": "main", "function": "startup_event", "line": 222}
{"timestamp": "2025-04-22T22:20:01.163248", "level": "INFO", "message": "Closing LLM client...", "module": "main", "function": "shutdown_event", "line": 228}
{"timestamp": "2025-04-22T22:20:01.165627", "level": "INFO", "message": "LLM client closed. Application shutting down.", "module": "main", "function": "shutdown_event", "line": 230}
{"timestamp": "2025-04-22T22:20:01.843365", "level": "INFO", "message": "\ud83d\ude80 Starting LLM Assistant API", "module": "main", "function": "startup_event", "line": 222}
{"timestamp": "2025-04-22T22:23:24.671047", "level": "INFO", "message": "Closing LLM client...", "module": "main", "function": "shutdown_event", "line": 228}
{"timestamp": "2025-04-22T22:23:24.673276", "level": "INFO", "message": "LLM client closed. Application shutting down.", "module": "main", "function": "shutdown_event", "line": 230}
{"timestamp": "2025-04-22T22:23:25.389425", "level": "INFO", "message": "\ud83d\ude80 Starting LLM Assistant API", "module": "main", "function": "startup_event", "line": 222}
{"timestamp": "2025-04-22T22:25:33.753618", "level": "INFO", "message": "Closing LLM client...", "module": "main", "function": "shutdown_event", "line": 228}
{"timestamp": "2025-04-22T22:25:33.757173", "level": "INFO", "message": "LLM client closed. Application shutting down.", "module": "main", "function": "shutdown_event", "line": 230}
{"timestamp": "2025-04-22T22:25:34.432354", "level": "INFO", "message": "\ud83d\ude80 Starting LLM Assistant API", "module": "main", "function": "startup_event", "line": 222}
{"timestamp": "2025-04-22T22:26:40.470974", "level": "INFO", "message": "Closing LLM client...", "module": "main", "function": "shutdown_event", "line": 228}
{"timestamp": "2025-04-22T22:26:40.473658", "level": "INFO", "message": "LLM client closed. Application shutting down.", "module": "main", "function": "shutdown_event", "line": 230}
{"timestamp": "2025-04-22T22:26:41.132431", "level": "INFO", "message": "\ud83d\ude80 Starting LLM Assistant API", "module": "main", "function": "startup_event", "line": 222}
{"timestamp": "2025-04-22T22:29:37.355956", "level": "INFO", "message": "Request: POST /assist - Time: 0.0065s", "module": "main", "function": "__call__", "line": 153}
{"timestamp": "2025-04-22T22:29:38.460665", "level": "INFO", "message": "Request: POST /assist - Time: 0.0007s", "module": "main", "function": "__call__", "line": 153}
{"timestamp": "2025-04-22T22:29:43.677353", "level": "INFO", "message": "Request: GET /todos - Time: 0.0020s", "module": "main", "function": "__call__", "line": 153}
{"timestamp": "2025-04-22T22:29:43.680858", "level": "INFO", "message": "Request: GET /todos - Time: 0.0010s", "module": "main", "function": "__call__", "line": 153}
{"timestamp": "2025-04-22T22:29:49.484923", "level": "INFO", "message": "Request: GET /todos - Time: 0.0018s", "module": "main", "function": "__call__", "line": 153}
{"timestamp": "2025-04-22T22:29:49.488250", "level": "INFO", "message": "Request: GET /todos - Time: 0.0008s", "module": "main", "function": "__call__", "line": 153}
{"timestamp": "2025-04-23T15:41:36.670918", "level": "INFO", "message": "\ud83d\ude80 Starting LLM Assistant API", "module": "main", "function": "startup_event", "line": 222}
{"timestamp": "2025-04-23T15:41:49.426865", "level": "INFO", "message": "Request: POST /assist - Time: 0.0069s", "module": "main", "function": "__call__", "line": 153}
{"timestamp": "2025-04-23T15:41:50.372007", "level": "INFO", "message": "Request: POST /assist - Time: 0.0017s", "module": "main", "function": "__call__", "line": 153}
{"timestamp": "2025-04-23T15:48:24.091650", "level": "INFO", "message": "Starting LLM Assistant API on 0.0.0.0:8000", "module": "main", "function": "<module>", "line": 528}
{"timestamp": "2025-04-23T15:48:24.762408", "level": "INFO", "message": "\ud83d\ude80 Starting LLM Assistant API", "module": "main", "function": "startup_event", "line": 222}
{"timestamp": "2025-04-23T15:49:32.676973", "level": "INFO", "message": "Request: POST /assist - Time: 0.0025s", "module": "main", "function": "__call__", "line": 153}
{"timestamp": "2025-04-23T15:50:01.545186", "level": "INFO", "message": "Request: POST /assist - Time: 0.0041s", "module": "main", "function": "__call__", "line": 153}
{"timestamp": "2025-04-23T15:51:11.194722", "level": "INFO", "message": "Request: POST /assist - Time: 0.0012s", "module": "main", "function": "__call__", "line": 153}
{"timestamp": "2025-04-23T15:52:59.642366", "level": "INFO", "message": "\ud83d\ude80 Starting LLM Assistant API", "module": "main", "function": "startup_event", "line": 222}
{"timestamp": "2025-04-23T15:53:30.300330", "level": "INFO", "message": "Request: POST /assist - Time: 0.0031s", "module": "main", "function": "__call__", "line": 153}
{"timestamp": "2025-04-23T15:55:18.255168", "level": "INFO", "message": "Request: POST /assist - Time: 0.0013s", "module": "main", "function": "__call__", "line": 153}
{"timestamp": "2025-04-23T15:55:27.984756", "level": "WARNING", "message": "Failed login attempt for user: testuser", "module": "main", "function": "login_for_access_token", "line": 281}
{"timestamp": "2025-04-23T15:55:27.985202", "level": "INFO", "message": "Request: POST /token - Time: 0.2520s", "module": "main", "function": "__call__", "line": 153}
{"timestamp": "2025-04-23T15:55:38.286294", "level": "WARNING", "message": "Failed login attempt for user: testuser", "module": "main", "function": "login_for_access_token", "line": 281}
{"timestamp": "2025-04-23T15:55:38.286713", "level": "INFO", "message": "Request: POST /token - Time: 0.2459s", "module": "main", "function": "__call__", "line": 153}
{"timestamp": "2025-04-23T15:55:39.655975", "level": "WARNING", "message": "Failed login attempt for user: testuser", "module": "main", "function": "login_for_access_token", "line": 281}
{"timestamp": "2025-04-23T15:55:39.656262", "level": "INFO", "message": "Request: POST /token - Time: 0.2385s", "module": "main", "function": "__call__", "line": 153}
{"timestamp": "2025-04-23T15:56:18.906392", "level": "INFO", "message": "Successful login for user: testuser", "module": "main", "function": "login_for_access_token", "line": 288}
{"timestamp": "2025-04-23T15:56:18.907181", "level": "INFO", "message": "Request: POST /token - Time: 0.2519s", "module": "main", "function": "__call__", "line": 153}
{"timestamp": "2025-04-23T15:56:23.181377", "level": "INFO", "message": "Request received", "module": "logger", "function": "log_request", "line": 74}
{"timestamp": "2025-04-23T15:56:23.858399", "level": "INFO", "message": "Response sent", "module": "logger", "function": "log_response", "line": 91}
{"timestamp": "2025-04-23T15:56:23.859101", "level": "INFO", "message": "Request: POST /assist - Time: 0.6819s", "module": "main", "function": "__call__", "line": 153}
{"timestamp": "2025-04-23T15:56:31.954406", "level": "INFO", "message": "Request received", "module": "logger", "function": "log_request", "line": 74}
{"timestamp": "2025-04-23T15:56:33.284490", "level": "INFO", "message": "Response sent", "module": "logger", "function": "log_response", "line": 91}
{"timestamp": "2025-04-23T15:56:33.285526", "level": "INFO", "message": "Request: POST /assist - Time: 1.3330s", "module": "main", "function": "__call__", "line": 153}
{"timestamp": "2025-04-23T15:56:42.364685", "level": "INFO", "message": "Request received", "module": "logger", "function": "log_request", "line": 74}
{"timestamp": "2025-04-23T15:56:44.441470", "level": "INFO", "message": "Response sent", "module": "logger", "function": "log_response", "line": 91}
{"timestamp": "2025-04-23T15:56:44.442494", "level": "INFO", "message": "Request: POST /assist - Time: 2.0801s", "module": "main", "function": "__call__", "line": 153}
{"timestamp": "2025-04-23T15:56:50.974044", "level": "INFO", "message": "Request received", "module": "logger", "function": "log_request", "line": 74}
{"timestamp": "2025-04-23T15:56:52.940116", "level": "INFO", "message": "Response sent", "module": "logger", "function": "log_response", "line": 91}
{"timestamp": "2025-04-23T15:56:52.941119", "level": "INFO", "message": "Request: POST /assist - Time: 1.9695s", "module": "main", "function": "__call__", "line": 153}
{"timestamp": "2025-04-23T15:56:58.752611", "level": "INFO", "message": "Request received", "module": "logger", "function": "log_request", "line": 74}
{"timestamp": "2025-04-23T15:57:00.523245", "level": "INFO", "message": "Response sent", "module": "logger", "function": "log_response", "line": 91}
{"timestamp": "2025-04-23T15:57:00.523888", "level": "INFO", "message": "Request: POST /assist - Time: 1.7731s", "module": "main", "function": "__call__", "line": 153}
{"timestamp": "2025-04-23T15:57:05.053037", "level": "INFO", "message": "Request received", "module": "logger", "function": "log_request", "line": 74}
{"timestamp": "2025-04-23T15:57:05.940990", "level": "INFO", "message": "Response sent", "module": "logger", "function": "log_response", "line": 91}
{"timestamp": "2025-04-23T15:57:05.941559", "level": "INFO", "message": "Request: POST /assist - Time: 0.8911s", "module": "main", "function": "__call__", "line": 153}
{"timestamp": "2025-04-23T15:57:22.414915", "level": "INFO", "message": "Request received", "module": "logger", "function": "log_request", "line": 74}
{"timestamp": "2025-04-23T15:57:23.656386", "level": "INFO", "message": "Response sent", "module": "logger", "function": "log_response", "line": 91}
{"timestamp": "2025-04-23T15:57:23.656975", "level": "INFO", "message": "Request: POST /assist - Time: 1.2448s", "module": "main", "function": "__call__", "line": 153}
{"timestamp": "2025-04-23T15:57:59.511328", "level": "INFO", "message": "Request received", "module": "logger", "function": "log_request", "line": 74}
{"timestamp": "2025-04-23T15:58:03.602255", "level": "INFO", "message": "Response sent", "module": "logger", "function": "log_response", "line": 91}
{"timestamp": "2025-04-23T15:58:03.602824", "level": "INFO", "message": "Request: POST /assist - Time: 4.0928s", "module": "main", "function": "__call__", "line": 153}
